{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import wandb\n",
    "from gridworld_ctf_mvp import GridworldCtf\n",
    "import os\n",
    "import time\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.distributions.categorical import Categorical\n",
    "import utils as ut\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_actions=8, alpha=0.0003):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(32*2*2 + 12, 256)\n",
    "        self.action_head = nn.Linear(256, n_actions)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "        self.optimizer = Adam(self.parameters(), lr=alpha)\n",
    "\n",
    "        self.action_head.weight.data.fill_(0.0)\n",
    "        self.value_head.weight.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, x2, print_shapes=False):\n",
    "        if print_shapes:\n",
    "            print(f'input shape: {x.shape}')\n",
    "\n",
    "        x = T.relu(self.conv1(x))\n",
    "        if print_shapes:\n",
    "            print(f'shape after conv 1: {x.shape}')\n",
    "        \n",
    "        x = T.relu(self.conv2(x))\n",
    "        if print_shapes:\n",
    "            print(f'shape after conv 2: {x.shape}')\n",
    "        \n",
    "        x = x.view(-1, 32*2*2)\n",
    "        if print_shapes:\n",
    "            print(f'shape after flattening: {x.shape}')\n",
    "\n",
    "        # Add in metadata\n",
    "        x = T.concat((x, x2), dim=1)\n",
    "        if print_shapes:\n",
    "            print(f'shape after adding metadata: {x.shape}')\n",
    "        \n",
    "        x = T.relu(self.fc1(x))\n",
    "        if print_shapes:\n",
    "            print(f'shape after FC layer: {x.shape}')\n",
    "        \n",
    "        action_scores = T.softmax(self.action_head(x), dim=-1)\n",
    "        if print_shapes:\n",
    "            print(f'action scores shape: {action_scores.shape}')\n",
    "\n",
    "        state_values = self.value_head(x)\n",
    "        if print_shapes:\n",
    "            print(f'state values shape: {state_values.shape}')\n",
    "\n",
    "        # Categorical below samples an action according to the  \n",
    "\n",
    "        return Categorical(action_scores).sample(), action_scores, state_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 9.0105e-01,  8.8071e-01,  9.9161e-02,  8.4424e-01,  1.3556e+00,\n",
      "            7.2633e-01],\n",
      "          [-6.0907e-01, -4.7004e-01, -6.2230e-01, -9.1117e-01,  1.4803e+00,\n",
      "           -5.1111e-01],\n",
      "          [-9.5306e-01,  5.3970e-01,  1.0382e+00,  3.0276e-01, -8.6736e-01,\n",
      "            6.6830e-01],\n",
      "          [ 2.1302e-01,  4.1352e-01, -1.4238e+00, -8.1579e-01,  1.2617e+00,\n",
      "            1.0434e+00],\n",
      "          [ 3.4611e-01, -3.8493e-01,  1.3110e-02,  4.7143e-01, -9.7373e-02,\n",
      "           -5.1921e-01],\n",
      "          [ 9.4590e-03,  1.2614e+00, -7.2829e-01, -1.0665e+00,  3.6353e-01,\n",
      "           -4.2868e-01]],\n",
      "\n",
      "         [[-2.7000e-01, -1.2078e+00, -7.0416e-01, -6.5911e-01,  4.1710e-01,\n",
      "           -1.7907e-01],\n",
      "          [ 2.8265e-01, -9.6100e-01,  4.8169e-01,  2.1039e-01,  5.5740e-01,\n",
      "            3.2149e-01],\n",
      "          [ 1.5748e+00, -8.4067e-01,  7.5193e-01, -6.7163e-01, -1.9647e+00,\n",
      "           -2.6164e-01],\n",
      "          [-3.6297e-01, -7.7888e-01,  8.1567e-01, -3.4932e-01,  4.8459e-02,\n",
      "            2.5136e-01],\n",
      "          [-2.1455e-02, -1.4126e+00,  5.4497e-01, -1.2504e+00, -1.1988e+00,\n",
      "            2.3737e-01],\n",
      "          [-8.5428e-01, -7.6377e-01,  3.5919e-02,  4.3445e-01, -4.3727e-01,\n",
      "            2.2824e-01]],\n",
      "\n",
      "         [[ 6.9799e-01,  6.6554e-01,  3.8053e-01,  1.2517e+00,  1.9357e+00,\n",
      "            1.0061e+00],\n",
      "          [ 2.2351e-01, -1.4803e-01,  1.5519e+00, -1.0374e+00,  5.4711e-01,\n",
      "            2.4044e-01],\n",
      "          [ 4.5816e-01, -1.7536e+00,  1.8363e+00, -4.5573e-01,  2.0060e+00,\n",
      "            1.2894e+00],\n",
      "          [-1.2559e-01,  1.4877e+00, -6.9015e-02, -1.3060e+00, -2.5649e-01,\n",
      "           -3.2111e-02],\n",
      "          [-1.8477e+00, -1.2261e+00, -2.7596e+00, -7.7223e-02,  2.9044e+00,\n",
      "           -3.6080e-02],\n",
      "          [ 1.0140e-01,  9.0745e-01, -1.8601e+00,  1.0611e+00, -5.9094e-01,\n",
      "           -1.0565e-01]],\n",
      "\n",
      "         [[ 6.2087e-01, -6.2744e-01,  5.1337e-02,  1.0441e+00, -6.7537e-01,\n",
      "            9.6718e-01],\n",
      "          [ 2.9027e-01,  4.4891e-04, -4.6516e-01,  1.6435e-01, -1.6562e+00,\n",
      "            1.6398e-02],\n",
      "          [ 1.1205e+00, -5.1255e-01,  1.2934e+00, -9.5770e-02,  7.2761e-01,\n",
      "           -5.2719e-01],\n",
      "          [ 5.0056e-01, -2.3590e+00,  5.6146e-01,  3.5246e-01,  2.1885e-01,\n",
      "            8.2183e-01],\n",
      "          [ 7.0204e-01,  8.3385e-03,  8.2085e-01,  5.2834e-01, -6.5737e-01,\n",
      "            7.5535e-02],\n",
      "          [-3.5287e-01, -2.3654e-01, -4.2508e-01, -1.0496e+00,  2.1858e-02,\n",
      "            7.5662e-01]],\n",
      "\n",
      "         [[ 3.8031e-01, -2.3412e+00, -1.5487e-01,  1.2188e+00, -9.4064e-01,\n",
      "            4.5337e-01],\n",
      "          [ 1.3618e+00,  7.9142e-01, -4.0556e-01,  5.4502e-01, -6.2415e-01,\n",
      "           -1.7688e+00],\n",
      "          [ 9.1697e-01, -5.6326e-01,  3.0061e-01,  7.9022e-01,  8.7943e-01,\n",
      "            4.3612e-01],\n",
      "          [-2.3658e-01,  1.4858e-01,  8.0158e-01,  4.3797e-01,  1.1968e+00,\n",
      "            6.5038e-01],\n",
      "          [ 1.7980e+00, -2.5085e-01,  9.5877e-01,  1.6587e-01, -1.0148e+00,\n",
      "           -2.2811e+00],\n",
      "          [ 1.2796e+00, -1.7242e+00, -5.2450e-01, -2.5824e-01,  3.4307e-01,\n",
      "           -1.3074e+00]]]])\n",
      "tensor([[ 0.3689, -0.3502,  0.9271, -0.3266, -1.1111,  1.7969,  0.3596, -0.2727,\n",
      "         -0.6234,  1.7586, -1.6784,  1.5188]])\n"
     ]
    }
   ],
   "source": [
    "pn = PolicyNet(n_channels=5)\n",
    "\n",
    "x = T.randn((1, 5, 6, 6))\n",
    "x2 = T.randn((1, 12))\n",
    "\n",
    "print(x)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 5, 6, 6])\n",
      "shape after conv 1: torch.Size([1, 16, 4, 4])\n",
      "shape after conv 2: torch.Size([1, 32, 2, 2])\n",
      "shape after flattening: torch.Size([1, 128])\n",
      "shape after adding metadata: torch.Size([1, 140])\n",
      "shape after FC layer: torch.Size([1, 256])\n",
      "action scores shape: torch.Size([1, 8])\n",
      "state values shape: torch.Size([1, 1])\n",
      "\n",
      "tensor([5])\n",
      "tensor([[0.1261, 0.1250, 0.1309, 0.1192, 0.1196, 0.1248, 0.1281, 0.1263]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0051]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action, probs, value = pn(x, x2, print_shapes=True)\n",
    "\n",
    "print()\n",
    "print(action)\n",
    "print(probs)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "            'GAME_MODE':'static',\n",
    "            'GRID_SIZE':6,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0}\n",
    "            },\n",
    "            'DROP_FLAG_WHEN_NO_HP':False\n",
    "        }\n",
    "\n",
    "env = GridworldCtf(**config)\n",
    "\n",
    "env.ACTION_SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.standardise_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.]]\n",
      "(1, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_local = env.get_env_metadata_local(0)\n",
    "print(metadata_local)\n",
    "print(metadata_local.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n",
      "(1, 28)\n"
     ]
    }
   ],
   "source": [
    "metadata_global = env.get_env_metadata_global([0, 0])\n",
    "print(metadata_global)\n",
    "print(metadata_global.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, \n",
    "                 n_epochs,\n",
    "                 batch_size,\n",
    "                 alpha,\n",
    "                 n_actions, \n",
    "                 grid_size=6, \n",
    "                 n_channels=5,\n",
    "                 c1=1.0,\n",
    "                 c2=0.01,\n",
    "                 epsilon=0.2,\n",
    "                 device=\"cpu\",\n",
    "                 normalise_rewards=True):\n",
    "\n",
    "        # Store attributes\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.n_actions = n_actions \n",
    "        self.grid_size = grid_size \n",
    "        self.n_channels = n_channels\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "        self.normalise_rewards = normalise_rewards\n",
    "\n",
    "        # Init annealing schedule and memory buffer\n",
    "        self.memory = []\n",
    "        \n",
    "        # Build actor and critic networks\n",
    "        self.policy_network = PolicyNet(alpha=self.alpha,\n",
    "                                        n_channels=n_channels,\n",
    "                                        n_actions=n_actions)\n",
    "\n",
    "    def choose_action(self, state, state2):\n",
    "        action, probs, value = self.policy_network(state, state2)\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def shuffle_memory(self):\n",
    "        np.random.shuffle(self.memory)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def compute_cumulative_rewards(self, gamma):\n",
    "        \"\"\"Given a buffer with states, policy action logits, rewards and terminations,\n",
    "        computes the cumulative rewards for each timestamp and substitutes them into the buffer.\"\"\"\n",
    "        curr_rew = 0.0\n",
    "\n",
    "        # Getting the average reward before normalizing (for logging and checkpointing)\n",
    "        avg_rew = np.mean([self.memory[i][-2] for i in range(len(self.memory))])\n",
    "\n",
    "        # Traversing the buffer on the reverse direction\n",
    "        for i in range(len(self.memory) - 1, -1, -1):\n",
    "            r, t = self.memory[i][-2], self.memory[i][-1]\n",
    "\n",
    "            if t:\n",
    "                curr_rew = r\n",
    "            else:\n",
    "                curr_rew = r + gamma * curr_rew\n",
    "\n",
    "            self.memory[i][-2] = curr_rew\n",
    "\n",
    "        # Normalizing cumulative rewards\n",
    "        if self.normalise_rewards:\n",
    "            mean = np.mean([self.memory[i][-2] for i in range(len(self.memory))])\n",
    "            std = np.std([self.memory[i][-2] for i in range(len(self.memory))]) + 1e-6\n",
    "            for i in range(len(self.memory)):\n",
    "                self.memory[i][-2] = (self.memory[i][-2] - mean) / std\n",
    "\n",
    "        return avg_rew\n",
    "\n",
    "    def get_losses(self, batch, annealing=1.0):\n",
    "        \"\"\"Returns the three loss terms for a given model and a given batch and additional parameters\"\"\"\n",
    "        # Getting old data\n",
    "        n = len(batch)\n",
    "        states = T.cat([batch[i][0] for i in range(n)])\n",
    "        states2 = T.cat([batch[i][1] for i in range(n)])\n",
    "        actions = T.cat([batch[i][2] for i in range(n)]).view(n, 1)\n",
    "\n",
    "        logits = T.cat([batch[i][3] for i in range(n)])\n",
    "        values = T.cat([batch[i][4] for i in range(n)])\n",
    "\n",
    "        cumulative_rewards = T.tensor([batch[i][-2] for i in range(n)]).view(-1, 1).float().to(self.device)\n",
    "\n",
    "        # Computing predictions with the new model\n",
    "        _, new_logits, new_values = self.choose_action(states, states2)\n",
    "\n",
    "        # Loss on the state-action-function / actor (L_CLIP)\n",
    "        advantages = cumulative_rewards - values\n",
    "        margin = self.epsilon * annealing\n",
    "        ratios = new_logits.gather(1, actions) / logits.gather(1, actions)\n",
    "\n",
    "        # print(f'states shape: {states.shape}')\n",
    "        # print(f'states2 shape: {states2.shape}')\n",
    "        # print(f'actions shape (before view): {T.cat([batch[i][2] for i in range(n)]).shape}')\n",
    "        # print(f'actions shape: {actions.shape}')\n",
    "        # print(f'logits shape: {logits.shape}')\n",
    "        # print(f'values shape: {values.shape}')\n",
    "        # print(f'logits: {batch[:][3]}')\n",
    "        # print(f'advantages shape: {advantages.shape}')\n",
    "        # print(f'ratios shape: {ratios.shape}')\n",
    "        # print(breakit)\n",
    "\n",
    "        l_clip = T.mean(\n",
    "            T.min(\n",
    "                T.cat(\n",
    "                    (ratios * advantages,\n",
    "                    T.clip(ratios, 1 - margin, 1 + margin) * advantages),\n",
    "                    dim=1),\n",
    "                dim=1\n",
    "            ).values\n",
    "        )\n",
    "\n",
    "        # Loss on the value-function / critic (L_VF)\n",
    "        l_vf = T.mean((cumulative_rewards - new_values) ** 2)\n",
    "\n",
    "        # Bonus for entropy of the actor\n",
    "        entropy_bonus = T.mean(T.sum(-new_logits * (T.log(new_logits + 1e-5)), dim=1))\n",
    "\n",
    "        return l_clip, l_vf, entropy_bonus\n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        # Shuffle memory bugger\n",
    "        self.shuffle_memory()\n",
    "\n",
    "        # Running optimization for a few epochs\n",
    "        for _ in range(self.n_epochs):\n",
    "            for batch_idx in range(len(self.memory) // self.batch_size):\n",
    "             \n",
    "                # Getting batch for this buffer\n",
    "                start = self.batch_size * batch_idx\n",
    "                end = start + self.batch_size if start + self.batch_size < len(self.memory) else -1\n",
    "                batch = self.memory[start:end]\n",
    "\n",
    "                # Zero-ing optimizers gradients\n",
    "                self.policy_network.optimizer.zero_grad()\n",
    "\n",
    "                # Getting the losses\n",
    "                l_clip, l_vf, entropy_bonus = self.get_losses(batch)\n",
    "\n",
    "                # Computing total loss and back-propagating it\n",
    "                loss = l_clip - self.c1 * l_vf + self.c2 * entropy_bonus\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimizing\n",
    "                self.policy_network.optimizer.step()\n",
    "                \n",
    "            # self.policy_network.scheduler.step()\n",
    "\n",
    "        # Clear memory buffer\n",
    "        self.clear_memory()\n",
    "\n",
    "        return loss, l_clip, l_vf, entropy_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOOpponent:\n",
    "    def __init__(self, \n",
    "                 n_channels,\n",
    "                 n_actions,\n",
    "                 weights):\n",
    "\n",
    "        # Build polcicy network\n",
    "        self.policy_network = PolicyNet(n_channels=n_channels, n_actions=n_actions)\n",
    "        self.policy_network.load_state_dict(weights)\n",
    "\n",
    "    def choose_action(self, state, state2):\n",
    "        action, probs, value = self.policy_network(state, state2)\n",
    "\n",
    "        probs = T.squeeze(probs)[action].item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.no_grad()\n",
    "def run_timestamps(env, \n",
    "                   agent, \n",
    "                   opponent,\n",
    "                   timestamps=128, \n",
    "                   render=False, \n",
    "                   device=\"cpu\",\n",
    "                   iteration=0):\n",
    "    \"\"\"Runs the given policy on the given environment for the given amount of timestamps.\n",
    "     Returns a buffer with state action transitions and rewards.\"\"\"\n",
    "    buffer_t1 = []\n",
    "    env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    # Running timestamps and collecting state, actions, rewards and terminations\n",
    "    for ts in range(timestamps):\n",
    "        # Collect actions for each agent\n",
    "        states = []\n",
    "        states2 = []\n",
    "        actions = []\n",
    "        action_logits = []\n",
    "        vals = []\n",
    "        for agent_idx in np.arange(env.N_AGENTS):\n",
    "            # Get global and local states\n",
    "            state_ = env.standardise_state(agent_idx, use_ego_state=True)\n",
    "            state = T.from_numpy(state_).float().to(device) #+ T.randn(*grid_state_local_.shape)\n",
    "\n",
    "            state2_ = env.get_env_metadata_local(agent_idx)\n",
    "            state2 = T.from_numpy(state2_).float().to(device)\n",
    "            \n",
    "            if env.AGENT_TEAMS[agent_idx]==0:\n",
    "                action, probs, value = agent.choose_action(state, state2)\n",
    "            else:\n",
    "                action, probs, value = opponent.choose_action(state, state2)\n",
    "                action = 0\n",
    "            \n",
    "            # Append actions and probs\n",
    "            states.append(state)\n",
    "            states2.append(state2)\n",
    "            actions.append(action)\n",
    "            action_logits.append(probs) #TODO: Check in other implementation if logits == probs\n",
    "            vals.append(value)\n",
    "\n",
    "        # Step the environment\n",
    "        _, reward, done = env.step(actions)\n",
    "\n",
    "        # Rendering / storing (s, a, r, t) in the buffer\n",
    "        if render and ts < 15 and iteration % 10 == 0 and iteration > 0:\n",
    "            # total_reward += reward[0]\n",
    "            env.render(sleep_time=0.01)\n",
    "            # print(f'step: {ts} \\treward: {reward} \\ttotal reward:{total_reward} \\n')\n",
    "\n",
    "        # Put the agent rollout data into the correct team buffer\n",
    "        for agent_idx in np.arange(env.N_AGENTS):\n",
    "            if env.AGENT_TEAMS[agent_idx]==0:\n",
    "                buffer_t1.append([states[agent_idx], \n",
    "                                states2[agent_idx],\n",
    "                                actions[agent_idx], \n",
    "                                action_logits[agent_idx], \n",
    "                                vals[agent_idx], \n",
    "                                reward[agent_idx], \n",
    "                                done])\n",
    "\n",
    "        # Resetting environment if episode terminated or truncated\n",
    "        if done:\n",
    "            env.reset()\n",
    "\n",
    "    return buffer_t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            'GAME_MODE':'static',\n",
    "            'GRID_SIZE':6,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0}\n",
    "            },\n",
    "            'DROP_FLAG_WHEN_NO_HP':False\n",
    "        }\n",
    "\n",
    "env = GridworldCtf(**config)\n",
    "\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 32\n",
    "alpha = 0.00003\n",
    "n_actions = 8\n",
    "\n",
    "agent_t1 = PPOAgent(n_epochs,\n",
    "                    batch_size,\n",
    "                    alpha,\n",
    "                    n_actions, \n",
    "                    grid_size=6, \n",
    "                    n_channels=5,\n",
    "                    c1=1.0,\n",
    "                    c2=0.01,\n",
    "                    device=\"cpu\")\n",
    "\n",
    "agent_t2 = PPOAgent(n_epochs,\n",
    "                    batch_size,\n",
    "                    alpha,\n",
    "                    n_actions, \n",
    "                    grid_size=6, \n",
    "                    n_channels=5,\n",
    "                    c1=1.0,\n",
    "                    c2=0.01,\n",
    "                    device=\"cpu\")\n",
    "\n",
    "buffer = run_timestamps(env, \n",
    "                        agent_t1, \n",
    "                        agent_t2,\n",
    "                        timestamps=128, \n",
    "                        render=False, \n",
    "                        device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]]]),\n",
       " tensor([[0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([7]),\n",
       " tensor([[0.1231, 0.1234, 0.1215, 0.1271, 0.1277, 0.1269, 0.1223, 0.1280]]),\n",
       " tensor([[0.0443]]),\n",
       " -1,\n",
       " False]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(env, \n",
    "                    agent, \n",
    "                    opponent, \n",
    "                    max_iterations, \n",
    "                    n_actors, \n",
    "                    horizon, \n",
    "                    gamma, \n",
    "                    device, \n",
    "                    use_mp=False,\n",
    "                    use_wandb=False,\n",
    "                    render=False):\n",
    "    \"\"\"Train the model on the given environment using multiple actors acting up to n timestamps.\"\"\"\n",
    "\n",
    "    # Training variables\n",
    "    max_reward = float(\"-inf\")\n",
    "\n",
    "    # global results\n",
    "    def collect_result(result):\n",
    "        results.append(result)\n",
    "\n",
    "    # Training loop\n",
    "    for iteration in range(max_iterations):\n",
    "        # Collecting timestamps for all actors with the current policy\n",
    "        if use_mp:\n",
    "            results = []\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            for _ in range(n_actors):\n",
    "                pool.apply_async(run_timestamps, \n",
    "                                args=(env, agent, opponent, horizon, render, device),\n",
    "                                callback=collect_result)   \n",
    "            pool.close()\n",
    "            pool.join()  # postpones the execution of next line of code until all processes in the queue are done.\n",
    "\n",
    "            # for i in range(n_actors):\n",
    "            #     buffer.extend(results[i])\n",
    "        else:\n",
    "            for _ in range(n_actors):\n",
    "                new_buffer_data = run_timestamps(env, agent, opponent, horizon, render, device, iteration)\n",
    "                agent.memory.extend(new_buffer_data)\n",
    "\n",
    "        # Computing cumulative rewards and shuffling the buffer\n",
    "        avg_rew = agent.compute_cumulative_rewards(gamma)\n",
    "\n",
    "        # Learn over epochs\n",
    "        loss, l_clip, l_vf, entropy_bonus = agent.learn()\n",
    "\n",
    "        log = f\"Iteration {iteration + 1} / {max_iterations}: \" \\\n",
    "              f\"Average Reward: {avg_rew:.2f}\\t\" \\\n",
    "              f\"Loss: {loss.item():.3f} \" \\\n",
    "              f\"(L_CLIP: {l_clip.item():.1f} | L_VF: {l_vf.item():.1f} | L_bonus: {entropy_bonus.item():.1f})\" \n",
    "        if avg_rew > max_reward:\n",
    "            # T.save(model.state_dict(), MODEL_PATH)\n",
    "            max_reward = avg_rew\n",
    "            log += \" --> Stored model with highest average reward\"\n",
    "        print(log)\n",
    "\n",
    "        # Logging information to W&B\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"average reward\": avg_rew,\n",
    "                \"loss (total)\": loss.item(),\n",
    "                \"loss (clip)\": l_clip.item(),\n",
    "                \"loss (vf)\": l_vf.item(),\n",
    "                \"loss (entropy bonus)\": entropy_bonus.item()\n",
    "            })\n",
    "\n",
    "    # Finishing W&B session\n",
    "    if use_wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found: Running on CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Gets the device (GPU if any) and logs the type\"\"\"\n",
    "    if T.cuda.is_available():\n",
    "        device = T.device(\"cuda\")\n",
    "        print(f\"Found GPU device: {T.cuda.get_device_name(device)}\")\n",
    "    # elif T.backends.mps.is_available() and T.backends.mps.is_built():\n",
    "    #      device = T.device(\"mps\")\n",
    "    #      print(f\"Found GPU device: {T.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        device = T.device(\"cpu\")\n",
    "        print(\"No GPU found: Running on CPU\")\n",
    "    return device\n",
    "\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing program arguments\n",
    "args = {}\n",
    "\n",
    "args[\"use_wandb\"] = False\n",
    "args[\"max_iterations\"] = 100\n",
    "args[\"n_actors\"] = 8\n",
    "args[\"horizon\"] = 512\n",
    "args[\"epsilon\"] = 0.1\n",
    "args[\"n_epochs\"] = 3\n",
    "args[\"batch_size\"] = 512\n",
    "args[\"lr\"] = 0.00003\n",
    "args[\"gamma\"] = 0.96\n",
    "args[\"c1\"] = 0.5\n",
    "args[\"c2\"] = 0.1\n",
    "args[\"n_test_episodes\"] = 1\n",
    "args[\"seed\"] = 0\n",
    "args[\"use_mp\"] = False\n",
    "args[\"render\"] = True\n",
    "args[\"normalise_rewards\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91 / 100: Average Reward: -1.00\tLoss: 0.725 (L_CLIP: -0.0 | L_VF: 1.5 | L_bonus: 0.0)\n",
      "Iteration 92 / 100: Average Reward: -1.00\tLoss: 1.223 (L_CLIP: 0.0 | L_VF: 2.4 | L_bonus: 0.0)\n",
      "Iteration 93 / 100: Average Reward: -1.00\tLoss: 1.360 (L_CLIP: 0.1 | L_VF: 2.6 | L_bonus: 0.0)\n",
      "Iteration 94 / 100: Average Reward: -1.00\tLoss: 1.341 (L_CLIP: 0.0 | L_VF: 2.7 | L_bonus: 0.0)\n",
      "Iteration 95 / 100: Average Reward: -1.00\tLoss: 0.913 (L_CLIP: 0.0 | L_VF: 1.8 | L_bonus: 0.0)\n",
      "Iteration 96 / 100: Average Reward: -1.00\tLoss: 0.246 (L_CLIP: -0.1 | L_VF: 0.6 | L_bonus: 0.0)\n",
      "Iteration 97 / 100: Average Reward: -1.00\tLoss: 1.240 (L_CLIP: 0.0 | L_VF: 2.4 | L_bonus: 0.0)\n",
      "Iteration 98 / 100: Average Reward: -1.00\tLoss: 1.567 (L_CLIP: 0.1 | L_VF: 3.0 | L_bonus: 0.0)\n",
      "Iteration 99 / 100: Average Reward: -1.00\tLoss: 1.006 (L_CLIP: 0.0 | L_VF: 2.0 | L_bonus: 0.0)\n",
      "Iteration 100 / 100: Average Reward: -1.00\tLoss: 1.341 (L_CLIP: 0.0 | L_VF: 2.6 | L_bonus: 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Setting seed\n",
    "# pl.seed_everything(args[\"seed\"])\n",
    "\n",
    "# Getting device\n",
    "device = get_device()\n",
    "\n",
    "# Init environment\n",
    "env_name = \"Multi-Agent-GW-CTF\"\n",
    "config = {\n",
    "            'GAME_MODE':'static',\n",
    "            'GRID_SIZE':6,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0}\n",
    "            },\n",
    "            'DROP_FLAG_WHEN_NO_HP':False,\n",
    "            'GLOBAL_REWARDS': False\n",
    "        }\n",
    "\n",
    "env = GridworldCtf(**config)\n",
    "env.AGENT_TYPE_DAMAGE = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0\n",
    "}\n",
    "env.REWARD_CAPTURE = 100\n",
    "env.REWARD_STEP = -1\n",
    "env.WINNING_POINTS = 100\n",
    "n_actions = env.ACTION_SPACE\n",
    "n_actions = 4\n",
    "\n",
    "# Agents\n",
    "agent = PPOAgent(args[\"n_epochs\"],\n",
    "                    args[\"batch_size\"],\n",
    "                    args[\"lr\"],\n",
    "                    n_actions, \n",
    "                    grid_size=6, \n",
    "                    n_channels=5,\n",
    "                    c1=args[\"c1\"],\n",
    "                    c2=args[\"c2\"],\n",
    "                    epsilon=args[\"epsilon\"],\n",
    "                    device=\"cpu\",\n",
    "                    normalise_rewards=args[\"normalise_rewards\"])\n",
    "\n",
    "opponent = PPOOpponent(n_channels=5,\n",
    "                        n_actions=n_actions,\n",
    "                        weights=agent.policy_network.state_dict())\n",
    "    \n",
    "# Starting a new Weights & Biases run\n",
    "if args[\"use_wandb\"]:\n",
    "    wandb.init(project=\"MARL-CTF-GW\",\n",
    "            name=f\"PPO - {env_name}\",\n",
    "            config={\n",
    "                \"env\": str(env),\n",
    "                \"grid_size\": env.GRID_SIZE,\n",
    "                \"n_agents\": env.N_AGENTS,\n",
    "                \"number of actors\": args[\"n_actors\"],\n",
    "                \"horizon\": args[\"horizon\"],\n",
    "                \"gamma\": args[\"gamma\"],\n",
    "                \"epsilon\": args[\"epsilon\"],\n",
    "                \"epochs\": args[\"n_epochs\"],\n",
    "                \"batch size\": args[\"batch_size\"],\n",
    "                \"learning rate\": args[\"lr\"],\n",
    "                \"c1\": args[\"c1\"],\n",
    "                \"c2\": args[\"c2\"]\n",
    "            })\n",
    "\n",
    "# Training\n",
    "training_loop(env, \n",
    "                agent, \n",
    "                opponent,\n",
    "                args[\"max_iterations\"], \n",
    "                args[\"n_actors\"], \n",
    "                args[\"horizon\"], \n",
    "                args[\"gamma\"], \n",
    "                device, \n",
    "                args[\"use_mp\"],\n",
    "                args[\"use_wandb\"],\n",
    "                args[\"render\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "max_steps = 500\n",
    "step_count = 0\n",
    "done= False\n",
    "device = 'cpu'\n",
    "env.reset()\n",
    "total_rewards = 0\n",
    "while not done:\n",
    "    step_count += 1\n",
    "\n",
    "    actions = []\n",
    "    for agent_idx in np.arange(env.N_AGENTS):\n",
    "        # Get global and local states\n",
    "        metadata_state_ = env.get_env_metadata_local(agent_idx) \n",
    "        metadata_state = T.from_numpy(metadata_state_).float().to(device)\n",
    "        \n",
    "        # Get global and local states\n",
    "        grid_state_ = env.standardise_state(agent_idx, use_ego_state=use_ego_state)\n",
    "        grid_state = T.from_numpy(grid_state_).float().to(device)\n",
    "\n",
    "        #curr_grid_state = env.standardise_state(agent_idx, use_ego_state=use_ego_state, scale_tiles=scale_tiles).reshape(*env_dims) + ut.add_noise(env_dims)\n",
    "\n",
    "        if env.AGENT_TEAMS[agent_idx]==0:\n",
    "            action, prob, val = agent.choose_action(grid_state, metadata_state)\n",
    "        else:\n",
    "            action, prob, val = opponent.choose_action(grid_state, metadata_state)\n",
    "            action = 0\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "    _, rewards, done = env.step(actions)\n",
    "    total_rewards += rewards[0]\n",
    "\n",
    "    for agent_idx in np.arange(env.N_AGENTS):\n",
    "        pass\n",
    "        # Add reward shaping\n",
    "        # rewards[agent_idx] += reward_shaping(agent_idx, env)\n",
    "    env.render(sleep_time=0.01)\n",
    "    print(step_count, rewards[0], total_rewards)\n",
    "\n",
    "    if step_count > max_steps:\n",
    "        done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/geoffrey.nightingale@contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step.ipynb Cell 28\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39mstandardise_state()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            'GAME_MODE':'static',\n",
    "            'GRID_SIZE':6,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0}\n",
    "            },\n",
    "            'DROP_FLAG_WHEN_NO_HP':False,\n",
    "            'GLOBAL_REWARDS': False\n",
    "        }\n",
    "\n",
    "env = GridworldCtf(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =env.standardise_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('torch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd690e671e1ea295c2072b349b255c40ccf2d1e4972d8ea9053035da215f2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
