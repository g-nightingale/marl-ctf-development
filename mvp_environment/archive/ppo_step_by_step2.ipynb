{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Original Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import wandb\n",
    "from gridworld_ctf_mvp import GridworldCtf\n",
    "import os\n",
    "import time\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.distributions.categorical import Categorical\n",
    "import utils as ut\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_actions=8, alpha=0.0003, device='cpu'):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(32*2*2 + 12, 256)\n",
    "        self.action_head = nn.Linear(256, n_actions)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "        self.optimizer = Adam(self.parameters(), lr=alpha)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, t, t2, print_shapes=False):\n",
    "        if print_shapes:\n",
    "            print(f'input shape: {t.shape}')\n",
    "\n",
    "        t = T.relu(self.conv1(t))\n",
    "        if print_shapes:\n",
    "            print(f'shape after conv 1: {t.shape}')\n",
    "        \n",
    "        t = T.relu(self.conv2(t))\n",
    "        if print_shapes:\n",
    "            print(f'shape after conv 2: {t.shape}')\n",
    "        \n",
    "        t = t.view(-1, 32*2*2)\n",
    "        if print_shapes:\n",
    "            print(f'shape after flattening: {t.shape}')\n",
    "\n",
    "        # Add in metadata\n",
    "        t = T.concat((t, t2), dim=1)\n",
    "        if print_shapes:\n",
    "            print(f'shape after adding metadata: {t.shape}')\n",
    "        \n",
    "        t = T.relu(self.fc1(t))\n",
    "        if print_shapes:\n",
    "            print(f'shape after FC 1 layer: {t.shape}')\n",
    "        \n",
    "        action_scores = T.softmax(self.action_head(t), dim=-1)\n",
    "        if print_shapes:\n",
    "            print(f'action scores shape: {action_scores.shape}')\n",
    "\n",
    "        state_values = self.value_head(t)\n",
    "        if print_shapes:\n",
    "            print(f'state values shape: {state_values.shape}')\n",
    "\n",
    "        # Categorical below samples an action according to the multinomial distribution\n",
    "        return Categorical(action_scores).sample(), action_scores, state_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_actions=8, alpha=0.0003, device='cpu'):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 16, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(16*4*4 + 12, 64)\n",
    "        self.action_head = nn.Linear(64, n_actions)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "        self.optimizer = Adam(self.parameters(), lr=alpha)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, t, t2, print_shapes=False):\n",
    "        if print_shapes:\n",
    "            print(f'input shape: {t.shape}')\n",
    "\n",
    "        t = T.relu(self.conv1(t))\n",
    "        if print_shapes:\n",
    "            print(f'shape after conv 1: {t.shape}')\n",
    "        \n",
    "        t = t.view(-1, 16*4*4)\n",
    "        if print_shapes:\n",
    "            print(f'shape after flattening: {t.shape}')\n",
    "\n",
    "        # Add in metadata\n",
    "        t = T.concat((t, t2), dim=1)\n",
    "        if print_shapes:\n",
    "            print(f'shape after adding metadata: {t.shape}')\n",
    "        \n",
    "        t = T.relu(self.fc1(t))\n",
    "        if print_shapes:\n",
    "            print(f'shape after FC 1 layer: {t.shape}')\n",
    "        \n",
    "        action_scores = T.softmax(self.action_head(t), dim=-1)\n",
    "        if print_shapes:\n",
    "            print(f'action scores shape: {action_scores.shape}')\n",
    "\n",
    "        state_values = self.value_head(t)\n",
    "        if print_shapes:\n",
    "            print(f'state values shape: {state_values.shape}')\n",
    "\n",
    "        # Categorical below samples an action according to the multinomial distribution\n",
    "        return Categorical(action_scores).sample(), action_scores, state_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "x1 = T.rand(1, 6, 6)\n",
    "x2 = T.rand(1, 12)\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1]),\n",
       " tensor([[0.1254, 0.1209, 0.1314, 0.1341, 0.1275, 0.1129, 0.1085, 0.1394]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.0360]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = PolicyNet(n_channels=1)\n",
    "pn.forward(x1, x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_actions=8, alpha=0.0003, device='cpu'):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(180 + 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.action_head = nn.Linear(64, n_actions)\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "        self.optimizer = Adam(self.parameters(), lr=alpha)\n",
    "        self.device = device\n",
    "\n",
    "        self.action_head.weight.data.fill_(0.0)\n",
    "        self.value_head.weight.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, x2, print_shapes=False):\n",
    "        # Flatten state\n",
    "        x = x.view(x.shape[0], 180)\n",
    "\n",
    "        # Add in metadata\n",
    "        x = T.concat((x, x2), dim=1)\n",
    "        x = T.relu(self.fc1(x))\n",
    "        x = T.relu(self.fc2(x))\n",
    "\n",
    "        # Get action scores and state values\n",
    "        action_scores = T.softmax(self.action_head(x), dim=-1)\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # Categorical below samples an action according to the  \n",
    "        return Categorical(action_scores).sample(), action_scores, state_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7, 4, 2, 6, 4, 4, 7, 4, 0, 5, 7, 7, 1, 2, 3, 5, 4, 2, 6, 1, 1, 6, 1, 6,\n",
       "         2, 7, 5, 3, 3, 1, 7, 3, 5, 1, 6, 1, 3, 5, 1, 0, 7, 5, 4, 3, 5, 6, 0, 1,\n",
       "         0, 1, 7, 0, 7, 6, 5, 0, 3, 1, 7, 7, 4, 2, 4, 4, 1, 1, 1, 5, 2, 0, 7, 3,\n",
       "         5, 4, 6, 7, 6, 7, 4, 7, 2, 0, 7, 4, 5, 1, 5, 7, 3, 5, 2, 6, 7, 1, 2, 2,\n",
       "         2, 5, 7, 4]),\n",
       " tensor([[0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355],\n",
       "         [0.1211, 0.1367, 0.1152, 0.1135, 0.1218, 0.1362, 0.1200, 0.1355]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727],\n",
       "         [0.0727]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = T.rand(100, 5, 6, 6)\n",
    "x2 = T.rand(100, 12)\n",
    "\n",
    "pn = PolicyNet()\n",
    "pn.forward(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.value_head.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size, device='cpu'):\n",
    "        self.states_grid = []\n",
    "        self.states_metadata = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states_grid)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return T.cat([t for t in self.states_grid]).to(self.device), \\\n",
    "                T.cat([t for t in self.states_metadata]).to(self.device), \\\n",
    "                np.array(self.probs), \\\n",
    "                np.array(self.vals), \\\n",
    "                np.array(self.actions), \\\n",
    "                np.array(self.rewards), \\\n",
    "                np.array(self.dones), \\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, \n",
    "                    state_grid, \n",
    "                    state_metadata, \n",
    "                    action, \n",
    "                    probs, \n",
    "                    vals, \n",
    "                    reward, \n",
    "                    done):\n",
    "        self.states_grid.append(state_grid)\n",
    "        self.states_metadata.append(state_metadata)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states_grid = []\n",
    "        self.states_metadata = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([19, 18,  2, 11, 13]), array([ 4, 10,  9, 16,  0]), array([ 7,  1, 12,  8, 17]), array([ 6, 15,  3,  5, 14])]\n"
     ]
    }
   ],
   "source": [
    "memory = PPOMemory(5)\n",
    "\n",
    "for x in range(20):\n",
    "    t = T.tensor([x, 1])\n",
    "    memory.store_memory(t, t, x, x, x, x, x)\n",
    "\n",
    "state_grid, \\\n",
    "state_metadata, \\\n",
    "old_prob_arr, \\\n",
    "vals_arr,\\\n",
    "action_arr, \\\n",
    "reward_arr, \\\n",
    "dones_arr, \\\n",
    "batches = memory.generate_batches()\n",
    "\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, \n",
    "                 n_actions, \n",
    "                 n_channels=4,\n",
    "                 gamma=0.95, \n",
    "                 alpha=0.00003, \n",
    "                 gae_lambda=0.95,\n",
    "                 policy_clip=0.2, \n",
    "                 batch_size=512, \n",
    "                 n_epochs=3):\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Build actor and critic networks\n",
    "        self.policy_network = PolicyNet(n_channels=n_channels,\n",
    "                                        alpha=self.alpha,\n",
    "                                        n_actions=n_actions)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def store_memory(self, \n",
    "                    state_grid,\n",
    "                    state_metadata,\n",
    "                    action, \n",
    "                    probs, \n",
    "                    vals, \n",
    "                    reward, \n",
    "                    done):\n",
    "        self.memory.store_memory(state_grid,\n",
    "                                    state_metadata,\n",
    "                                    action,\n",
    "                                    probs, \n",
    "                                    vals, \n",
    "                                    reward, \n",
    "                                    done)\n",
    "\n",
    "\n",
    "    def choose_action(self, state, state2):\n",
    "        action, probs, value = self.policy_network(state, state2)\n",
    "\n",
    "        probs = T.squeeze(probs)[action].item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_grid, \\\n",
    "            state_metadata, \\\n",
    "            old_prob_arr, \\\n",
    "            vals_arr,\\\n",
    "            action_arr, \\\n",
    "            reward_arr, \\\n",
    "            dones_arr, \\\n",
    "            batches = self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    # a[t] = discount * (r[t] + gamma * values[t+1] * (1 - done) - values[k])\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*(1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.policy_network.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.policy_network.device)\n",
    "            for batch in batches:\n",
    "\n",
    "                grid_states = state_grid[batch]\n",
    "                metadata_states = state_metadata[batch]\n",
    "\n",
    "                old_probs_batch = T.tensor(old_prob_arr[batch]).to(self.policy_network.device)\n",
    "                actions_batch = T.tensor(action_arr[batch], dtype=T.int64).to(self.policy_network.device)\n",
    "\n",
    "                _, new_probs_batch, critic_value = self.policy_network(grid_states, metadata_states)\n",
    "                \n",
    "                actions_vw = actions_batch.view(len(batch), 1)\n",
    "\n",
    "                prob_ratio = new_probs_batch.gather(1, actions_vw) / old_probs_batch.view(len(batch), 1)\n",
    "\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.policy_network.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.policy_network.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()     \n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOOpponent:\n",
    "    def __init__(self, \n",
    "                 n_channels,\n",
    "                 n_actions,\n",
    "                 weights):\n",
    "\n",
    "        # Build polcicy network\n",
    "        self.policy_network = PolicyNet(n_channels=n_channels, n_actions=n_actions)\n",
    "        self.policy_network.load_state_dict(weights)\n",
    "\n",
    "    def choose_action(self, state, state2):\n",
    "        action, probs, value = self.policy_network(state, state2)\n",
    "\n",
    "        probs = T.squeeze(probs)[action].item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shaping(agent_idx, env):\n",
    "\n",
    "    DISTANCE_PENALTY = -1.0\n",
    "    max_distance_from_flags = env.GRID_SIZE - 2\n",
    "    \n",
    "    if env.has_flag[agent_idx]:\n",
    "        own_flag_pos = env.FLAG_POSITIONS[0]\n",
    "        distance_to_own_flag = env.agent_distance_to_xy(agent_idx, own_flag_pos)\n",
    "        reward = distance_to_own_flag/max_distance_from_flags * DISTANCE_PENALTY\n",
    "    else:\n",
    "        opp_flag_pos = env.FLAG_POSITIONS[1]\n",
    "        distance_to_opp_flag = env.agent_distance_to_xy(agent_idx, opp_flag_pos)\n",
    "        reward = distance_to_opp_flag/max_distance_from_flags * DISTANCE_PENALTY\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env,\n",
    "            agent,\n",
    "            opponent,\n",
    "            n_episodes,\n",
    "            n_actors,\n",
    "            learning_steps=20,\n",
    "            max_steps=1000,\n",
    "            use_ego_state=False,\n",
    "            verbose_episodes=5,\n",
    "            device='cpu'):\n",
    "    \"\"\"\n",
    "    Train PPO agent.\n",
    "    \"\"\"\n",
    "\n",
    "    step_count = 0\n",
    "    done_count = 0\n",
    "\n",
    "    training_metrics = {\n",
    "        \"score_history\": [], \n",
    "        \"losses\": [],\n",
    "        \"team_1_captures\": [], \n",
    "        \"team_2_captures\": [],\n",
    "        \"team_1_tags\": [],\n",
    "        \"team_2_tags\": [],\n",
    "        \"episode_step_counts\": [],\n",
    "        \"agent_tag_count\": defaultdict(list),\n",
    "        \"agent_flag_captures\": defaultdict(list),\n",
    "        \"agent_blocks_laid\": defaultdict(list),\n",
    "        \"agent_blocks_mined\": defaultdict(list),\n",
    "        \"agent_avg_distance_to_own_flag\": defaultdict(list),\n",
    "        \"agent_avg_distance_to_opp_flag\": defaultdict(list),\n",
    "        \"agent_health_pickups\": defaultdict(list),\n",
    "    }\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        # Get rollout\n",
    "        env.reset()\n",
    "        done = False\n",
    "        episode_step_count = 0\n",
    "        score = 0\n",
    "\n",
    "        while not done: \n",
    "            step_count += 1\n",
    "            episode_step_count += 1\n",
    "\n",
    "            # Collect actions for each agent\n",
    "            grid_states = []\n",
    "            metadata_states = []\n",
    "            actions = []\n",
    "            probs = []\n",
    "            vals = []\n",
    "            for agent_idx in np.arange(env.N_AGENTS):\n",
    "                # Get global and local states\n",
    "                metadata_state_ = env.get_env_metadata_local(agent_idx) \n",
    "                metadata_state = T.from_numpy(metadata_state_).float().to(device)\n",
    "                \n",
    "                # Get global and local states\n",
    "                grid_state_ = env.standardise_state(agent_idx, use_ego_state=use_ego_state)\n",
    "                grid_state = T.from_numpy(grid_state_).float().to(device)\n",
    "\n",
    "                #curr_grid_state = env.standardise_state(agent_idx, use_ego_state=use_ego_state, scale_tiles=scale_tiles).reshape(*env_dims) + ut.add_noise(env_dims)\n",
    "\n",
    "                if env.AGENT_TEAMS[agent_idx]==0:\n",
    "                    action, prob, val = agent.choose_action(grid_state, metadata_state)\n",
    "                else:\n",
    "                    action, prob, val = opponent.choose_action(grid_state, metadata_state)\n",
    "                    action = 0\n",
    "\n",
    "                # Append actions and probs\n",
    "                grid_states.append(grid_state)\n",
    "                metadata_states.append(metadata_state)\n",
    "                actions.append(action)\n",
    "                probs.append(prob)\n",
    "                vals.append(val)\n",
    "\n",
    "            # Step the environment\n",
    "            _, rewards, done = env.step(actions)\n",
    "\n",
    "            \n",
    "            # Increment score\n",
    "            score += sum(rewards)\n",
    "\n",
    "            # Store each agent experiences\n",
    "            for agent_idx in np.arange(env.N_AGENTS):\n",
    "                # Add reward shaping\n",
    "                # rewards[agent_idx] += reward_shaping(agent_idx, env)\n",
    "\n",
    "                # Append replay buffer\n",
    "                if env.AGENT_TEAMS[agent_idx]==0:\n",
    "                    agent.store_memory(grid_states[agent_idx],\n",
    "                                        metadata_states[agent_idx],\n",
    "                                        actions[agent_idx], \n",
    "                                        probs[agent_idx],\n",
    "                                        vals[agent_idx],\n",
    "                                        rewards[agent_idx], \n",
    "                                        done\n",
    "                                        )\n",
    "\n",
    "            if done or episode_step_count > max_steps:\n",
    "                done_count += 1 * done\n",
    "                done = True\n",
    "\n",
    "        # Learning\n",
    "        if step_count % learning_steps == 0 and step_count > agent.batch_size:\n",
    "            loss = agent.learn().detach().numpy()\n",
    "        else:\n",
    "            loss = 0.0\n",
    "\n",
    "        # Termination -> Append metrics\n",
    "        if done or episode_step_count > max_steps:\n",
    "            training_metrics['losses'].append((loss, 0.0))   \n",
    "            training_metrics['team_1_captures'].append(env.metrics['team_points'][0])\n",
    "            training_metrics['team_2_captures'].append(env.metrics['team_points'][1])\n",
    "            training_metrics['team_1_tags'].append(env.metrics['tag_count'][0])\n",
    "            training_metrics['team_2_tags'].append(env.metrics['tag_count'][1])\n",
    "            for agent_idx in range(env.N_AGENTS):\n",
    "                training_metrics[\"agent_tag_count\"][agent_idx].append(env.metrics['agent_tag_count'][agent_idx])\n",
    "                training_metrics[\"agent_flag_captures\"][agent_idx].append(env.metrics['agent_flag_captures'][agent_idx])\n",
    "                training_metrics[\"agent_blocks_laid\"][agent_idx].append(env.metrics['agent_blocks_laid'][agent_idx])\n",
    "                training_metrics[\"agent_blocks_mined\"][agent_idx].append(env.metrics['agent_blocks_mined'][agent_idx])\n",
    "                training_metrics[\"agent_avg_distance_to_own_flag\"][agent_idx].append(env.metrics['agent_total_distance_to_own_flag'][agent_idx]/env.env_step_count)\n",
    "                training_metrics[\"agent_avg_distance_to_opp_flag\"][agent_idx].append(env.metrics['agent_total_distance_to_opp_flag'][agent_idx]/env.env_step_count)\n",
    "                training_metrics[\"agent_health_pickups\"][agent_idx].append(env.metrics['agent_health_pickups'][agent_idx])\n",
    "\n",
    "        training_metrics['score_history'].append(score)\n",
    "        training_metrics['episode_step_counts'].append(episode_step_count)\n",
    "\n",
    "        if i % verbose_episodes == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"episode: {i+1} \\ttotal step count: {step_count} \\tepisode step count: {episode_step_count} \\tscore: {score} \\taverage score: {np.mean(training_metrics['score_history'][-100:])} \\tdone count: {done_count}\")\n",
    "\n",
    "        # if avg_score > best_score:\n",
    "        #     best_score = avg_score\n",
    "        #     agent.save_models()\n",
    "\n",
    "    return training_metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PPOMemory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/geoffrey.nightingale@contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb Cell 21\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m use_ego_state \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m verbose_episodes \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m agent \u001b[39m=\u001b[39m PPOAgent(n_actions\u001b[39m=\u001b[39;49mn_actions, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                     n_channels\u001b[39m=\u001b[39;49mn_channels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                     alpha\u001b[39m=\u001b[39;49malpha, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                     policy_clip\u001b[39m=\u001b[39;49mpolicy_clip,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                     n_epochs\u001b[39m=\u001b[39;49mn_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m opponent \u001b[39m=\u001b[39m PPOOpponent(n_channels\u001b[39m=\u001b[39mn_channels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                        n_actions\u001b[39m=\u001b[39mn_actions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                         weights\u001b[39m=\u001b[39magent\u001b[39m.\u001b[39mpolicy_network\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m training_metrics \u001b[39m=\u001b[39m train_ppo(env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                         agent,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                         opponent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                         verbose_episodes\u001b[39m=\u001b[39mverbose_episodes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                         device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/geoffrey.nightingale@contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb Cell 21\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Build actor and critic networks\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_network \u001b[39m=\u001b[39m PolicyNet(n_channels\u001b[39m=\u001b[39mn_channels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                                 alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                 n_actions\u001b[39m=\u001b[39mn_actions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m PPOMemory(batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PPOMemory' is not defined"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "            'GAME_MODE':'static',\n",
    "            'GRID_SIZE':6,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0}\n",
    "            },\n",
    "            'DROP_FLAG_WHEN_NO_HP':False,\n",
    "            'GLOBAL_REWARDS': False\n",
    "        }\n",
    "\n",
    "env = GridworldCtf(**config)\n",
    "env.AGENT_TYPE_DAMAGE = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0\n",
    "}\n",
    "env.REWARD_CAPTURE = 100\n",
    "env.REWARD_STEP = -1\n",
    "env.WINNING_POINTS = 100\n",
    "n_actions = env.ACTION_SPACE\n",
    "n_actions = 4\n",
    "\n",
    "n_channels = 5\n",
    "n_actors = 4\n",
    "n_episodes = 500\n",
    "learning_steps = 8\n",
    "max_steps = 128\n",
    "batch_size = 128\n",
    "n_epochs = 3\n",
    "alpha = 0.0003\n",
    "policy_clip = 0.2\n",
    "softmax_temp = 0.9\n",
    "use_ego_state = True\n",
    "\n",
    "verbose_episodes = 1\n",
    "\n",
    "\n",
    "agent = PPOAgent(n_actions=n_actions, \n",
    "                    n_channels=n_channels,\n",
    "                    alpha=alpha, \n",
    "                    policy_clip=policy_clip,\n",
    "                    n_epochs=n_epochs)\n",
    "\n",
    "    \n",
    "opponent = PPOOpponent(n_channels=n_channels,\n",
    "                       n_actions=n_actions,\n",
    "                        weights=agent.policy_network.state_dict())\n",
    "\n",
    "training_metrics = train_ppo(env,\n",
    "                        agent,\n",
    "                        opponent,\n",
    "                        n_episodes,\n",
    "                        n_actors,\n",
    "                        learning_steps=learning_steps,\n",
    "                        max_steps=max_steps,\n",
    "                        use_ego_state=use_ego_state,\n",
    "                        verbose_episodes=verbose_episodes,\n",
    "                        device='cpu')\n",
    "\n",
    "ut.plot_training_performance(training_metrics)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/geoffrey.nightingale@contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb Cell 22\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# Add reward shaping\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# rewards[agent_idx] += reward_shaping(agent_idx, env)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender(sleep_time\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(step_count, rewards[\u001b[39m0\u001b[39m], total_rewards)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geoffrey.nightingale%40contino.io/Documents/code/marl-ctf-development/mvp_environment/ppo_step_by_step2.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m step_count \u001b[39m>\u001b[39m max_steps:\n",
      "File \u001b[0;32m~/Documents/code/marl-ctf-development/mvp_environment/gridworld_ctf_mvp.py:885\u001b[0m, in \u001b[0;36mGridworldCtf.render\u001b[0;34m(self, sleep_time, ego_state_agent)\u001b[0m\n\u001b[1;32m    882\u001b[0m ax\u001b[39m.\u001b[39mset_yticks(np\u001b[39m.\u001b[39marange(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m, env_plot\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[1;32m    883\u001b[0m ax\u001b[39m.\u001b[39mset_yticklabels([])\n\u001b[0;32m--> 885\u001b[0m plt\u001b[39m.\u001b[39;49mshow()\n\u001b[1;32m    887\u001b[0m \u001b[39m# Sleep if desired\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[39mif\u001b[39;00m (sleep_time \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) :\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/pyplot.py:421\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mexplicitly there.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m _get_backend_mod()\u001b[39m.\u001b[39;49mshow(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         display(\n\u001b[1;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[1;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/IPython/core/formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    175\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/IPython/core/formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/IPython/core/formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/backend_bases.py:2314\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[1;32m   2309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[1;32m   2310\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m   2311\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[1;32m   2312\u001b[0m     )\n\u001b[1;32m   2313\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2314\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m   2316\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2317\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/figure.py:3071\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3071\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3072\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3074\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3075\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/axes/_base.py:3107\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3105\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3107\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3108\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3110\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[1;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[1;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[1;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[0;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[1;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:555\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    552\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_scalar_alpha()\n\u001b[1;32m    553\u001b[0m     output_alpha \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample alpha channel\u001b[39;00m\n\u001b[1;32m    554\u001b[0m         \u001b[39mself\u001b[39m, A[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m], out_shape, t, alpha\u001b[39m=\u001b[39malpha)\n\u001b[0;32m--> 555\u001b[0m     output \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample rgb channels\u001b[39;49;00m\n\u001b[1;32m    556\u001b[0m         \u001b[39mself\u001b[39;49m, _rgb_to_rgba(A[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, :\u001b[39m3\u001b[39;49m]), out_shape, t, alpha\u001b[39m=\u001b[39;49malpha)\n\u001b[1;32m    557\u001b[0m     output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m output_alpha  \u001b[39m# recombine rgb and alpha\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/matplotlib/image.py:207\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n\u001b[0;32m--> 207\u001b[0m _image\u001b[39m.\u001b[39;49mresample(data, out, transform,\n\u001b[1;32m    208\u001b[0m                 _interpd_[interpolation],\n\u001b[1;32m    209\u001b[0m                 resample,\n\u001b[1;32m    210\u001b[0m                 alpha,\n\u001b[1;32m    211\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filternorm(),\n\u001b[1;32m    212\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filterrad())\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 500\n",
    "step_count = 0\n",
    "done= False\n",
    "device = 'cpu'\n",
    "env.reset()\n",
    "total_rewards = 0\n",
    "while not done:\n",
    "    step_count += 1\n",
    "\n",
    "    actions = []\n",
    "    for agent_idx in np.arange(env.N_AGENTS):\n",
    "        # Get global and local states\n",
    "        metadata_state_ = env.get_env_metadata_local(agent_idx) \n",
    "        metadata_state = T.from_numpy(metadata_state_).float().to(device)\n",
    "        \n",
    "        # Get global and local states\n",
    "        grid_state_ = env.standardise_state(agent_idx, use_ego_state=use_ego_state)\n",
    "        grid_state = T.from_numpy(grid_state_).float().to(device)\n",
    "\n",
    "        #curr_grid_state = env.standardise_state(agent_idx, use_ego_state=use_ego_state, scale_tiles=scale_tiles).reshape(*env_dims) + ut.add_noise(env_dims)\n",
    "\n",
    "        if env.AGENT_TEAMS[agent_idx]==0:\n",
    "            action, prob, val = agent.choose_action(grid_state, metadata_state)\n",
    "        else:\n",
    "            action, prob, val = opponent.choose_action(grid_state, metadata_state)\n",
    "            action = 0\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "    _, rewards, done = env.step(actions)\n",
    "    total_rewards += rewards[0]\n",
    "\n",
    "    for agent_idx in np.arange(env.N_AGENTS):\n",
    "        pass\n",
    "        # Add reward shaping\n",
    "        # rewards[agent_idx] += reward_shaping(agent_idx, env)\n",
    "    env.render(sleep_time=0.01)\n",
    "    print(step_count, rewards[0], total_rewards)\n",
    "\n",
    "    if step_count > max_steps:\n",
    "        done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You win!, total score 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.play(0)\n",
    "reward_shaping(0, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.standardise_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.standardise_state(1, reverse_grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You win!, total score 85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.play(0)\n",
    "env.standardise_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.standardise_state(0, use_multi_channel=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 6, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.standardise_state(0, use_multi_channel=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 1.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0.]]]]),\n",
       " tensor([[0., 1., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 1., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 1., 1.,  ..., 0., 1., 1.],\n",
       "         ...,\n",
       "         [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 1., 1.,  ..., 1., 1., 1.]]),\n",
       " array([0.12516835, 0.13510759, 0.1236278 , 0.13509709, 0.12552142,\n",
       "        0.13473327, 0.12529534, 0.1170216 , 0.13482422, 0.12104146,\n",
       "        0.11967351, 0.12314358, 0.12227   , 0.12505352, 0.12386142,\n",
       "        0.12463164, 0.13456501, 0.112358  , 0.119606  , 0.12362824,\n",
       "        0.11292286, 0.11596399, 0.13530572, 0.13376537, 0.12129483,\n",
       "        0.13376109, 0.14527167, 0.11805823, 0.13433728, 0.10910665,\n",
       "        0.12662312, 0.1266166 , 0.10928121, 0.12510893, 0.11832076,\n",
       "        0.12532344, 0.11748876, 0.11828878, 0.12769035, 0.11549404,\n",
       "        0.10870547, 0.11714726, 0.13248801, 0.12723757, 0.10844136,\n",
       "        0.1265192 , 0.12311133, 0.1097975 , 0.123758  , 0.14365634,\n",
       "        0.12327013, 0.11738051, 0.12830782, 0.12791069, 0.1263707 ,\n",
       "        0.11603949, 0.143363  , 0.12443962, 0.11625759, 0.1189692 ,\n",
       "        0.11154921, 0.12488237, 0.1354848 , 0.11689268, 0.14752066,\n",
       "        0.11581945, 0.13466042, 0.11631259, 0.1212507 , 0.11927974,\n",
       "        0.11373691, 0.12603134, 0.13464905, 0.13422494, 0.11325275,\n",
       "        0.12507737, 0.13450065, 0.13255608, 0.11683086, 0.11668615,\n",
       "        0.11594334, 0.14181627, 0.13223119, 0.11075123, 0.12824893,\n",
       "        0.10965864, 0.10987486, 0.14249027, 0.11798643, 0.13510759,\n",
       "        0.13513799, 0.11620689, 0.12439044, 0.11105824, 0.11721256,\n",
       "        0.13372351, 0.12446465, 0.13352765, 0.11285186, 0.13364628,\n",
       "        0.12269316, 0.11945784, 0.12262703, 0.11541054, 0.11541054,\n",
       "        0.11541054, 0.11347584, 0.12538059, 0.12428989, 0.12640497,\n",
       "        0.12661195, 0.13409916, 0.13463132, 0.1446237 , 0.1184527 ,\n",
       "        0.11549187, 0.12360054, 0.12454554, 0.11259036, 0.11637557,\n",
       "        0.1464947 , 0.11545145, 0.12561585, 0.11552398, 0.1237998 ,\n",
       "        0.11708698, 0.12458853, 0.11647834, 0.12428486, 0.12393934,\n",
       "        0.14538698, 0.11307456, 0.12524047, 0.1189545 , 0.1462127 ,\n",
       "        0.12342542, 0.12551656, 0.11931351, 0.14583527, 0.13423829,\n",
       "        0.11553376, 0.11934771, 0.11571063, 0.11300206, 0.1149478 ,\n",
       "        0.11955108, 0.12464151, 0.11300206, 0.12461554, 0.13362066,\n",
       "        0.14422552, 0.13400607, 0.12430649, 0.11643544, 0.12320177,\n",
       "        0.14504518, 0.11668938, 0.11558805, 0.11937618, 0.11621148,\n",
       "        0.11556361, 0.12361081, 0.14503527, 0.14468455, 0.11350054,\n",
       "        0.12324581, 0.1353038 , 0.13298146, 0.1235683 , 0.14412941,\n",
       "        0.12498483, 0.1353038 , 0.14744988, 0.11623696, 0.13422421,\n",
       "        0.123261  , 0.1236522 , 0.11578117, 0.12370333, 0.11626736,\n",
       "        0.11968475, 0.13411181, 0.13408732, 0.13411181, 0.12479089,\n",
       "        0.12275755, 0.11977726, 0.13404736, 0.12277681, 0.12426104,\n",
       "        0.12860963, 0.1163666 , 0.14156061, 0.11555601, 0.12409665,\n",
       "        0.11015589, 0.12959874, 0.11708852, 0.12688619, 0.12968646,\n",
       "        0.14189623]),\n",
       " array([-0.4331595 , -0.4331595 , -0.45601755, -0.45403516, -0.48997203,\n",
       "        -0.48108619, -0.48401675, -0.47237125, -0.47405118, -0.46689299,\n",
       "        -0.46498865, -0.51488173, -0.48159456, -0.45801947, -0.45474556,\n",
       "        -0.45241272, -0.45757365, -0.46144682, -0.44827241, -0.45892474,\n",
       "        -0.46940571, -0.43650743, -0.46276206, -0.50238287, -0.48564374,\n",
       "        -0.48466346, -0.48260084, -0.49349597, -0.47094607, -0.48354757,\n",
       "        -0.48827249, -0.48135889, -0.49948275, -0.49653348, -0.47296286,\n",
       "        -0.49948275, -0.50689864, -0.47502482, -0.52205235, -0.4637866 ,\n",
       "        -0.47103509, -0.49498653, -0.52205235, -0.509552  , -0.4962337 ,\n",
       "        -0.51234138, -0.47404087, -0.48233354, -0.44974816, -0.46027958,\n",
       "        -0.46022213, -0.47937754, -0.47368708, -0.4791452 , -0.45732266,\n",
       "        -0.45659959, -0.48112583, -0.49273929, -0.4331595 , -0.46467727,\n",
       "        -0.48111215, -0.47342369, -0.46411684, -0.51488173, -0.4964869 ,\n",
       "        -0.47077382, -0.47144234, -0.44004163, -0.43459314, -0.44084188,\n",
       "        -0.43010572, -0.47280324, -0.47319132, -0.50270379, -0.48030233,\n",
       "        -0.51795518, -0.4608379 , -0.49340436, -0.49340436, -0.49110219,\n",
       "        -0.4624249 , -0.45661098, -0.47441465, -0.47988325, -0.50082421,\n",
       "        -0.52226198, -0.54325408, -0.50673163, -0.47535548, -0.4331595 ,\n",
       "        -0.45601755, -0.47973764, -0.5107556 , -0.4817577 , -0.47788614,\n",
       "        -0.48995933, -0.47943527, -0.4748745 , -0.44675386, -0.46711129,\n",
       "        -0.43136275, -0.43288812, -0.43288812, -0.42660424, -0.42660424,\n",
       "        -0.42660424, -0.42660424, -0.42660424, -0.44382456, -0.46711129,\n",
       "        -0.49122497, -0.45901683, -0.46963894, -0.46963894, -0.45901683,\n",
       "        -0.49122497, -0.49122497, -0.48256576, -0.49251837, -0.47869852,\n",
       "        -0.49122497, -0.50026035, -0.46766815, -0.44432867, -0.43390194,\n",
       "        -0.50745487, -0.44120023, -0.43274173, -0.4694131 , -0.44710028,\n",
       "        -0.43977013, -0.44056842, -0.46458834, -0.43894282, -0.4916147 ,\n",
       "        -0.4650076 , -0.45338002, -0.43411174, -0.45919463, -0.45627686,\n",
       "        -0.4608379 , -0.44641921, -0.46933219, -0.44939947, -0.46890342,\n",
       "        -0.43288323, -0.44297636, -0.44939947, -0.4916147 , -0.47141039,\n",
       "        -0.45096594, -0.43456751, -0.4425166 , -0.45096594, -0.43588784,\n",
       "        -0.46939176, -0.4655349 , -0.43588784, -0.43588784, -0.47113454,\n",
       "        -0.43708351, -0.43708351, -0.43074307, -0.43281138, -0.43588784,\n",
       "        -0.45084319, -0.48295337, -0.49749354, -0.49459356, -0.47976872,\n",
       "        -0.48625219, -0.48295337, -0.52437681, -0.47061867, -0.45919463,\n",
       "        -0.46799308, -0.49087852, -0.4848074 , -0.47280324, -0.47280324,\n",
       "        -0.43010572, -0.43010572, -0.43222985, -0.43010572, -0.44187868,\n",
       "        -0.43049729, -0.43125111, -0.43125111, -0.4496994 , -0.48062018,\n",
       "        -0.49239865, -0.49017662, -0.47871578, -0.47486928, -0.46329436,\n",
       "        -0.47486928, -0.52420521, -0.49799502, -0.46024022, -0.45771229,\n",
       "        -0.45771229]),\n",
       " array([5, 1, 5, 1, 5, 1, 5, 7, 1, 3, 6, 3, 3, 5, 5, 2, 1, 4, 6, 2, 4, 7,\n",
       "        1, 1, 3, 1, 0, 6, 1, 4, 5, 5, 4, 5, 6, 5, 7, 6, 5, 7, 4, 6, 1, 5,\n",
       "        4, 5, 3, 4, 3, 0, 3, 6, 2, 2, 5, 7, 0, 3, 7, 6, 4, 5, 1, 7, 0, 7,\n",
       "        1, 7, 3, 6, 4, 5, 1, 1, 4, 2, 1, 1, 6, 7, 7, 0, 1, 4, 5, 4, 4, 0,\n",
       "        6, 1, 1, 7, 2, 4, 7, 1, 2, 1, 4, 1, 3, 6, 3, 7, 7, 7, 4, 5, 2, 5,\n",
       "        5, 1, 1, 0, 6, 7, 3, 2, 4, 7, 0, 7, 2, 7, 3, 7, 2, 7, 3, 2, 0, 4,\n",
       "        5, 6, 0, 2, 5, 6, 0, 1, 7, 6, 7, 4, 7, 6, 2, 4, 5, 1, 0, 1, 2, 7,\n",
       "        3, 0, 7, 7, 6, 7, 7, 2, 0, 0, 4, 3, 1, 1, 3, 0, 5, 1, 0, 7, 1, 3,\n",
       "        3, 7, 2, 7, 6, 1, 1, 1, 2, 3, 6, 1, 3, 2, 2, 7, 0, 7, 3, 4, 2, 7,\n",
       "        5, 2, 0]),\n",
       " array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, 99, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True]),\n",
       " [array([ 14, 149,  75,  29, 182,  81,  13, 147, 105,  77,   2,  97, 167,\n",
       "          45,  50,  64, 190, 122, 188, 139, 137,  11, 184, 107, 200, 101,\n",
       "          67, 117, 164, 179, 159,  40,  38,  17, 136, 134,  90,   3,  69,\n",
       "          54, 142,  58, 192, 168, 143,   6, 141, 155,  23,  62,  32, 112,\n",
       "         196, 103, 194,  41,  63,  99, 110,  82, 111, 160,  47, 120,  76,\n",
       "          66,  95,   0, 171, 162,  71,   7, 109,   9, 191, 150,  80,  88,\n",
       "         166,  46, 181,  98, 158,  35, 126,  12,   1,  70,  24,  89,  86,\n",
       "         140,  43,  26, 185, 125, 135, 169,  39, 102, 131, 175, 146, 165,\n",
       "          68, 133,  37, 183, 178,  34, 129, 116,  93,   4,  30, 113, 180,\n",
       "         104,  33,  61,  79, 170,  20,  18, 119, 124, 173, 128, 186,   8,\n",
       "          57,  85,  36, 199, 132,  55,  27, 152,  22, 148,   5,  16,  28,\n",
       "          96,  21, 123,  83,  44,  94, 161,  15, 177, 108,  73,  87,  51,\n",
       "         118, 100, 145,  19,  60,  52,  31,  25, 172, 127,  56, 195, 198,\n",
       "         130,  78, 106,  84, 115, 151, 197, 187, 174,  48,  53,  10, 144,\n",
       "         156, 189,  74,  49, 176,  59, 138, 154, 121, 153, 114, 157,  65,\n",
       "         163,  42,  92,  72, 193,  91])])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_grid, \\\n",
    "state_metadata, \\\n",
    "old_prob_arr, \\\n",
    "vals_arr,\\\n",
    "action_arr, \\\n",
    "reward_arr, \\\n",
    "dones_arr, \\\n",
    "batches = agent.memory.generate_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_idx = np.where(dones_arr==True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_metadata[done_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, 99, -1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_arr[done_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.zeros((2,) + (4, 4))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 6, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.standardise_state(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [2., 4.],\n",
      "        [3., 1.]])\n",
      "tensor([0., 2., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = T.Tensor([1, 2, 3])\n",
    "y = T.Tensor([0, 4, 1])\n",
    "\n",
    "z = T.cat((x.view(3, 1), y.view(3, 1)), dim=1)\n",
    "\n",
    "print(z)\n",
    "\n",
    "zz = T.min(z,dim=1)\n",
    "\n",
    "print(zz.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('torch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd690e671e1ea295c2072b349b255c40ccf2d1e4972d8ea9053035da215f2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
