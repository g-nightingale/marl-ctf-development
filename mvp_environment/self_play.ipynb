{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# import gym\n",
    "from gridworld_ctf_mvp import GridworldCtf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "from agent_network import agent\n",
    "\n",
    "import wandb\n",
    "\n",
    "from ppo import PPOTrainer\n",
    "from agent_network import Agent\n",
    "from metrics_logger import MetricsLogger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### League Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duel(env, agent, opponent, max_steps=256, render=False, sleep_time=0.01):\n",
    "    \"\"\"\n",
    "    Duelling algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    step_count = 0\n",
    "    done = False\n",
    "    device = 'cpu'\n",
    "    env.reset()\n",
    "\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "\n",
    "        actions = []\n",
    "        \n",
    "        for agent_idx in np.arange(env.N_AGENTS):\n",
    "            # Get global and local states\n",
    "            if env.AGENT_TEAMS[agent_idx]==0:\n",
    "                grid_state = torch.tensor(env.standardise_state(agent_idx, use_ego_state=True), dtype=torch.float32).to(device)\n",
    "                metadata_state = torch.tensor(env.get_env_metadata_local(agent_idx), dtype=torch.float32).to(device)\n",
    "                action = agent.get_action(grid_state, metadata_state)\n",
    "            else:\n",
    "                grid_state = torch.tensor(env.standardise_state(agent_idx, use_ego_state=True, reverse_grid=True), dtype=torch.float32).to(device)\n",
    "                metadata_state = torch.tensor(env.get_env_metadata_local(agent_idx), dtype=torch.float32).to(device)\n",
    "                action = opponent.get_action(grid_state, metadata_state)\n",
    "                action = env.get_reversed_action(action)\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        _, _, done = env.step(actions)\n",
    "\n",
    "        if render:\n",
    "            env.render(sleep_time=sleep_time)\n",
    "            print(step_count, env.metrics['team_flag_captures'][0], env.metrics['team_flag_captures'][1])\n",
    "\n",
    "        if step_count > max_steps:\n",
    "            done = True\n",
    "\n",
    "    return env.metrics['team_flag_captures'][0], env.metrics['team_flag_captures'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig():\n",
    "    def __init__(self):\n",
    "        #---------- Overall config\n",
    "        self.wandb_project_name = \"MARL-CTF-Test\"\n",
    "        self.exp_name = \"Agent metrics logging\"\n",
    "        self.use_wandb_selfplay = True\n",
    "        self.use_wandb_ppo = False\n",
    "        self.seed = 42\n",
    "\n",
    "        #---------- Self-play config\n",
    "        self.number_of_iterations = 50\n",
    "        self.number_of_duels = 3\n",
    "        self.n_main_agents = 5\n",
    "        self.n_exploiters = 3\n",
    "        self.n_historical_agents = 20\n",
    "        self.main_agent_update_interval = 5\n",
    "        self.exploiter_update_interval = 2\n",
    "        self.historical_update_interval = 2\n",
    "        self.max_win_rate_diff = 1.0\n",
    "\n",
    "        #---------- Environment config\n",
    "        self.use_ego_state = True\n",
    "        self.env_config = {\n",
    "            'GRID_SIZE':8,\n",
    "            'AGENT_CONFIG':{\n",
    "                0: {'team':0, 'type':0},\n",
    "                1: {'team':1, 'type':0},\n",
    "                2: {'team':0, 'type':1},\n",
    "                3: {'team':1, 'type':1},\n",
    "            },\n",
    "            'GAME_STEPS': 256,\n",
    "            'USE_ADJUSTED_REWARDS': False,\n",
    "            'RANDOMISE_FLAG_POSITIONS': False,\n",
    "            'HOME_FLAG_CAPTURE': True,\n",
    "            'ENABLE_OBSTACLES': False,\n",
    "            'MAX_BLOCK_TILE_PCT': 0.05\n",
    "        }\n",
    "\n",
    "        #---------- PPO Config\n",
    "        self.learning_rate = 0.0003\n",
    "        self.total_timesteps = 10000\n",
    "        self.torch_deterministic = True\n",
    "        self.cuda = True\n",
    "        self.wandb_entity = None\n",
    "        self.capture_video = False\n",
    "        self.parallel_rollouts = False\n",
    "        self.num_envs = 4\n",
    "        self.num_agents = 1\n",
    "        self.num_steps = 256\n",
    "        self.anneal_lr = False\n",
    "        self.gae = True\n",
    "        self.gamma = 0.95\n",
    "        self.gae_lambda = 0.96\n",
    "        self.num_minibatches = 4\n",
    "        self.update_epochs = 4\n",
    "        self.norm_adv = False\n",
    "        self.clip_coef = 0.2\n",
    "        self.clip_vloss = True\n",
    "        self.ent_coef = 0.01\n",
    "        self.vf_coef = 0.5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.target_kl = None\n",
    "        self.batch_size = int(self.num_envs * self.num_steps * self.num_agents)\n",
    "        self.minibatch_size = int(self.batch_size // self.num_minibatches)\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        #---------- Metrics config\n",
    "\n",
    "args = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_opponent(iteration, agent_idx, main_agents, exploiters, historical_agents, weights=(0.5, 0.3, 0.2)):\n",
    "    \"\"\"\n",
    "    Select agents for training.\n",
    "    \"\"\"\n",
    "    agent_types = ['main', 'exploiter', 'historical']\n",
    "    \n",
    "    # Adjust weights if we do not have any historical agents yet\n",
    "    if len(historical_agents) == 0:\n",
    "        weights = (weights[0]/sum(weights[:2]), weights[1]/sum(weights[:2]), 0.0)\n",
    "\n",
    "    # Choose opponent\n",
    "    opponent_type = np.random.choice(agent_types, p=weights)\n",
    "    opponent_idx = np.random.choice(np.arange(len(main_agents)) if opponent_type == 'main' else\n",
    "                            np.arange(len(exploiters)) if opponent_type == 'exploiter' else\n",
    "                            np.arange(len(historical_agents)))\n",
    "                            \n",
    "    if opponent_type == 'main':\n",
    "        opponent = main_agents[opponent_idx] \n",
    "    elif opponent_type == 'exploiter':\n",
    "        opponent = exploiters[opponent_idx]\n",
    "    else:\n",
    "        opponent = historical_agents[opponent_idx]\n",
    "\n",
    "    # Ensure agent1 and agent2 are different\n",
    "    while main_agents[agent_idx] == opponent:\n",
    "        opponent_type = np.random.choice(agent_types, p=weights)\n",
    "        opponent_idx = np.random.choice(np.arange(len(main_agents)) if opponent_type == 'main' else\n",
    "                                np.arange(len(exploiters)) if opponent_type == 'exploiter' else\n",
    "                                np.arange(len(historical_agents)))\n",
    "        if opponent_type == 'main':\n",
    "            opponent = main_agents[opponent_idx] \n",
    "        elif opponent_type == 'exploiter':\n",
    "            opponent = exploiters[opponent_idx]\n",
    "        else:\n",
    "            opponent = historical_agents[opponent_idx]\n",
    "\n",
    "    print(f'Iteration: {iteration} Training main agent {agent_idx} vs {opponent_type} agent {opponent_idx}')\n",
    "    return opponent\n",
    "\n",
    "def select_agents(main_agents, exploiters, historical_agents, weights=(0.5, 0.3, 0.2)):\n",
    "    \"\"\"\n",
    "    Select agents for training.\n",
    "    \"\"\"\n",
    "    agent_types = ['main', 'exploiter', 'historical']\n",
    "    \n",
    "    # Adjust weights if we do not have any historical agents yet\n",
    "    if len(historical_agents) == 0:\n",
    "        weights = (weights[0]/sum(weights[:2]), weights[1]/sum(weights[:2]), 0.0)\n",
    "\n",
    "    # Choose main agent\n",
    "    agent_idx = rng.choice(len(main_agents))\n",
    "    agent = main_agents[agent_idx]\n",
    "    \n",
    "    opponent_type = np.random.choice(agent_types, p=weights)\n",
    "    opponent_idx = np.random.choice(np.arange(len(main_agents)) if opponent_type == 'main' else\n",
    "                            np.arange(len(exploiters)) if opponent_type == 'exploiter' else\n",
    "                            np.arange(len(historical_agents)))\n",
    "                            \n",
    "    if opponent_type == 'main':\n",
    "        opponent = main_agents[opponent_idx] \n",
    "    elif opponent_type == 'exploiter':\n",
    "        opponent = exploiters[opponent_idx]\n",
    "    else:\n",
    "        opponent = historical_agents[opponent_idx]\n",
    "\n",
    "    # Ensure agent1 and agent2 are different\n",
    "    while agent == opponent:\n",
    "        opponent_type = np.random.choice(agent_types, p=weights)\n",
    "        opponent_idx = np.random.choice(np.arange(len(main_agents)) if opponent_type == 'main' else\n",
    "                                np.arange(len(exploiters)) if opponent_type == 'exploiter' else\n",
    "                                np.arange(len(historical_agents)))\n",
    "        if opponent_type == 'main':\n",
    "            opponent = main_agents[opponent_idx] \n",
    "        elif opponent_type == 'exploiter':\n",
    "            opponent = exploiters[opponent_idx]\n",
    "        else:\n",
    "            opponent = historical_agents[opponent_idx]\n",
    "\n",
    "    print(f'Training main agent {agent_idx} vs {opponent_type} agent {opponent_idx}')\n",
    "    return agent, opponent\n",
    "\n",
    "def update_main_agents(main_agents, win_rate_dict, max_win_rate_diff=args.max_win_rate_diff):\n",
    "    \"\"\"\n",
    "    Update agents based on relative performance.\n",
    "    \"\"\"\n",
    "    # Get the probability of adding each main agent, based on their win rates\n",
    "    win_rates = np.array([v for v in win_rate_dict.values()])\n",
    "    probs = win_rates/ sum(win_rates)\n",
    "\n",
    "    if max(win_rates) - min(win_rates) > max_win_rate_diff:\n",
    "        # Get the main agent to add\n",
    "        agent_to_add = np.argmax(probs)\n",
    "        agent_to_remove = np.argmin(probs)\n",
    "\n",
    "        print(f'Copying agent {agent_to_add}, removing agent {agent_to_remove}')\n",
    "\n",
    "        # Add the main agent to the historical agents pool\n",
    "        new_agent = main_agents[agent_to_add]\n",
    "        main_agents.append(new_agent)\n",
    "\n",
    "        # Maintain the pool size\n",
    "        del main_agents[agent_to_remove]\n",
    "\n",
    "def update_exploiters(main_agents, exploiters):\n",
    "    \"\"\"\n",
    "    Train exploiters against main agents.\n",
    "    \"\"\"\n",
    "    for exploiter_idx, exploiter in enumerate(exploiters):\n",
    "        print(f'Updating exploiter {exploiter_idx}')\n",
    "        main_agent = random.choice(main_agents)  # Select a main agent to target\n",
    "\n",
    "        ppotrainer.train_ppo(args, env, exploiter, main_agent)\n",
    "        clear_output()\n",
    "\n",
    "def update_historical_agents(main_agents, historical_agents, win_rate_dict, max_pool_size=args.n_historical_agents):\n",
    "    \"\"\"\n",
    "    Add main agents to history.\n",
    "    \"\"\"\n",
    "    # Get the probability of adding each main agent, based on their win rates\n",
    "    win_rates = np.array([v for v in win_rate_dict.values()])\n",
    "    probs = win_rates / sum(win_rates)\n",
    "\n",
    "    # Get the main agent to add\n",
    "    main_agent_to_add = random.choices(main_agents, probs, k=1)[0]\n",
    "\n",
    "    # Add the main agent to the historical agents pool\n",
    "    historical_agents.append(main_agent_to_add)\n",
    "\n",
    "    # Maintain the pool size\n",
    "    if len(historical_agents) > max_pool_size:\n",
    "        historical_agents.pop(0)  # Remove the oldest agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 49 Duelling...\n",
      "Win rates:\n",
      "{0: 0.20481927710843378, 1: 0.28915662650602414, 2: 0.5542168674698794, 3: 0.40963855421686735, 4: 0.3253012048192771}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n",
      "/Users/geoffrey.nightingale@contino.io/opt/anaconda3/envs/torch-env/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_type_blocks_laid (agent:0, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:0, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:1, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:1, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:2, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:2, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:3, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:3, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:4, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_laid (agent:4, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:0, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:0, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:1, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:1, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:2, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:2, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:3, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:3, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:4, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_blocks_mined (agent:4, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_flag_captures (agent:0, agent type:0)</td><td>▁▁▁▂▂▂▂▃▂▃▆▇██▇█▇▇▅▅▆▄▄▃▆▅▅▆▅▃▅▅▄▃▃▄▃▃▃▅</td></tr><tr><td>agent_type_flag_captures (agent:0, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_flag_captures (agent:1, agent type:0)</td><td>▁▁▂▂▃▃▂▃▃▄▅▅█▅▆▅█▆▆▆▆▆▆▆▅▅▆▇▆▅▇▇▇▆▇▅▆▆▆▆</td></tr><tr><td>agent_type_flag_captures (agent:1, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_flag_captures (agent:2, agent type:0)</td><td>▁▁▁▁▃▃▄▇▇▇▆█▆▇▇▆▆▆▆▆▅▅▇▇▅▆▆▅▇▇▆▆▄▄▅▆▇▆▆█</td></tr><tr><td>agent_type_flag_captures (agent:2, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_flag_captures (agent:3, agent type:0)</td><td>▁▁▂▁▂▂▃▃▄▃▄▅▄▆▄▅▅▅▆▆▅▆▇▇▇█▇█▇▆▆▇▆▅▆▇▇▆▇▆</td></tr><tr><td>agent_type_flag_captures (agent:3, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_flag_captures (agent:4, agent type:0)</td><td>▁▁▁▁▂▂▁▁▁▂▂▂▃▂▂▃▂▃▄▅▇▆█▆▅▇▇▇█▇▆▆▆▄▆▇▆▇▇▆</td></tr><tr><td>agent_type_flag_captures (agent:4, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:0, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:0, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:1, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:1, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:2, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:2, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:3, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:3, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:4, agent type:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_health_pickups (agent:4, agent type:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_type_respawn_tag_count (agent:0, agent type:0)</td><td>▁▁▁▂▁▁▂▂▂▂▃▄▅▅▅▅▆▆▄▄▆▄▄▄█▆▆▆▅▃▅▅▅▃▄▅▄▄▄▆</td></tr><tr><td>agent_type_respawn_tag_count (agent:0, agent type:1)</td><td>▂▁▁▁▂▂▄▄▃▄▅▅▅▆▄▃▂▂▃▅▄▅▄▅▄▅▇▅▆▅▆▇▅▅▆▅▇▇█▆</td></tr><tr><td>agent_type_respawn_tag_count (agent:1, agent type:0)</td><td>▁▁▁▂▁▁▁▂▂▃▃▃▅▄▅▄▆▆▆▄▅▅▅▅▅▅▆█▅▆▇▇▆▆▇▅▇█▇▇</td></tr><tr><td>agent_type_respawn_tag_count (agent:1, agent type:1)</td><td>▂▁▃▃▃▃▄▃▃▆▅▅▅▄▅▄▄▄▄▄▅▆▆▆▇▇▆▆▆▄▅▆▇▆▅▇█▆▆█</td></tr><tr><td>agent_type_respawn_tag_count (agent:2, agent type:0)</td><td>▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▃▄▃▃▃▃▃▅▅▄▅▆▅▆▆▆▆▄▄▆▅▇▇▅█</td></tr><tr><td>agent_type_respawn_tag_count (agent:2, agent type:1)</td><td>▁▂▁▂▃▂▃▂▄▄▅▄▃▄▄▃▃▃▃▃▃▂▄▅▄▄▄▆▅▅▅▅▃▂▂▅█▇▅█</td></tr><tr><td>agent_type_respawn_tag_count (agent:3, agent type:0)</td><td>▁▁▁▁▂▁▁▂▂▁▂▃▃▃▃▃▃▃▄▄▅▅▄▆▆▇▇▇▆▆▆▆▄▄▅▅▇█▆▆</td></tr><tr><td>agent_type_respawn_tag_count (agent:3, agent type:1)</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▃▄▄▅▃▃▅▅▆▆▆▆▇█▇▇▆▅▅▅█▆▅▄</td></tr><tr><td>agent_type_respawn_tag_count (agent:4, agent type:0)</td><td>▁▁▁▁▂▁▂▂▂▂▂▂▃▂▃▃▂▃▄▅▅▄▅▄▅▇▇▇▆▆▆▆▅▄▅▆▆█▆▆</td></tr><tr><td>agent_type_respawn_tag_count (agent:4, agent type:1)</td><td>▁▁▁▁▃▃▂▂▃▂▃▃▃▃▃▂▂▂▂▃▃▅▅▅▄▄▄▃▅▃▄▅▅▄▅▆▇▅▇█</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:0, agent type:0)</td><td>▃▅▇▆▆▆▇▃▆▄▄▃▃▃▂▁▃▃▆▇▃▄▄▄▂▅▄▂▄█▁▁▃▅▅▃▆▅▇▅</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:0, agent type:1)</td><td>▅▇█▆▄▄▅▃▆▅▅▄▃▄▃▂▂▁▆█▅▅▃▅▄▅▆▃▃█▄▅▃▆▆▃█▇▇▂</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:1, agent type:0)</td><td>▆█▄▅▇▇▆▆▄▃▄▅▁▃▄▁▃▅▆▅▄▆▄▄▆▆▃▂▆▄▃▃▄▃▄▆▄▃▄▅</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:1, agent type:1)</td><td>▇█▅▅▆▆▅▆▄▃▄▄▂▂▃▁▃▃▂▃▄▅▄▆▇█▆▆▇▃▄▄▃▄▆█▆▆▆▆</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:2, agent type:0)</td><td>▃▄▅▅▃▄▄▄▂▃▄▄▅▅▄▄▃▅▃▅▆▆▆▅▆▆▅▆▆▄▆▆▇█▅▄▁▂▃▃</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:2, agent type:1)</td><td>█▇█▇▄▄▇▅▁▁▂▂▄▃▃▃▃▂▂▂▂▂▃▄▇▆▃▄▅▃▄▄▂▃▂▅▇▆▃▆</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:3, agent type:0)</td><td>██▇█▇▇█▆▄▆▅▅▄▅▇▅▁▄▃▆▇▇▅▅▅▄▄▄▆▅▃▄▂▄▃▃▅▅▄▅</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:3, agent type:1)</td><td>█▇██▅▇█▃▁▅▃▂▄▆▅▃▃▆▅▄▂▂▄▆▇▇▇▆▇█▇▅▅▄▃▅▆▅▄▂</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:4, agent type:0)</td><td>▄▆▇▇▆▄██▇▇▆▇▇█▆▅▄▅▅▅▁▄▂▄▄▂▃▄▅▅▅▅▄▅▄▃▄▂▅▅</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:4, agent type:1)</td><td>▇▇▇▆▆▂▆▆▆▅▄▆▃▆▄▂▅▁▁▅▃▆▅▆▆▅▅▂▃▁▂▅▄▃▅▄▇▅██</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:0, agent type:0)</td><td>▅▃▁▄▄▄▄▇▃▆▄▆▅▄▆▇▆▆▄▄▆▇▅▄█▅▇█▇▅█▇▇▆▇▇▆▇▆▆</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:0, agent type:1)</td><td>▃▂▁▃▅▅▄▆▄▅▅▆▆▆▆▇██▅▃▆▆▇▆▇▆▅▇▇▄▆▅▇▅▅▇▃▄▄█</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:1, agent type:0)</td><td>▁▁▅▅▃▃▄▃▆▆▅▄▆▅▄▇▅▄▃▄▆▅▆▅▅▆▇▇▇█▇▆▇▇▆▅▆▇█▆</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:1, agent type:1)</td><td>▁▁▄▄▃▃▄▃▅▆▆▅▇▇▆█▇▇▇▇▇▆▇▆▄▅▆▆▅▇█▇▇█▇▅▆▇▇▆</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:2, agent type:0)</td><td>▂▂▁▂▃▁▁▁▂▂▁▂▂▂▂▂▃▂▄▃▄▄▃▃▄▆▅▂▃▄▃▄▇█▆▅▅▆▆▅</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:2, agent type:1)</td><td>▁▂▂▃▄▄▃▄▇▆▆▆▆▆▆▇▆▇▇█▇▇▇▆▄▄▆▅▆▇▆▆▇██▅▃▄▆▄</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:3, agent type:0)</td><td>▁▂▃▃▄▃▃▄▅▅▅▅▆▅▄▆▇▅▆▅▄▄▅▅▆▆▆▇▅▇▇▆██▇▇▅▆▆▆</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:3, agent type:1)</td><td>▁▂▂▂▅▃▃▆▇▆▆▇▆▅▆▇▇▅▆▇▇▇▇▅▄▅▅▅▄▄▅▆▆▆▇▆▅▆▇█</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:4, agent type:0)</td><td>▂▂▁▁▄▆▃▂▄▆▅▄▅▃▆▆█▇▅▆█▆█▆▆█▇▆▆▅▆▆▇▆▇▇▇▇▆▆</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:4, agent type:1)</td><td>▁▁▂▂▃▆▄▄▄▅▅▅▆▄▆▇▅█▇▅▆▄▅▅▅▆▇█▇██▆▆▇▅▆▃▅▃▂</td></tr><tr><td>agent_type_total_tag_count (agent:0, agent type:0)</td><td>▁▁▁▂▁▂▂▂▂▃▄▄▅▆▆▆▆▆▅▅▆▄▅▅█▇▇▇▆▄▆▆▆▄▅▆▅▅▄▆</td></tr><tr><td>agent_type_total_tag_count (agent:0, agent type:1)</td><td>▂▁▁▂▃▃▄▅▃▄▅▅▅▆▄▄▂▃▄▅▅▅▄▅▅▅▇▆▆▅▆▇▅▅▆▆▆▇█▇</td></tr><tr><td>agent_type_total_tag_count (agent:1, agent type:0)</td><td>▁▁▂▂▁▂▁▂▂▃▃▃▅▅▅▄▅▆▅▄▅▅▅▅▅▅▆▇▅▅▇▇▆▆▇▆▆█▇▇</td></tr><tr><td>agent_type_total_tag_count (agent:1, agent type:1)</td><td>▂▁▃▃▃▃▄▃▃▅▅▅▅▄▆▄▄▅▄▄▅▆▅▆▇▇▆▆▆▅▅▆█▆▆▇█▆▆█</td></tr><tr><td>agent_type_total_tag_count (agent:2, agent type:0)</td><td>▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▃▄▃▃▃▅▅▄▅▅▅▆▆▅▆▄▄▆▅█▇▅█</td></tr><tr><td>agent_type_total_tag_count (agent:2, agent type:1)</td><td>▁▂▁▂▃▂▃▃▄▄▅▄▄▅▄▃▄▃▃▃▃▃▄▅▄▅▅▆▆▅▅▅▃▂▃▅▇▇▅█</td></tr><tr><td>agent_type_total_tag_count (agent:3, agent type:0)</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▆▆▇▆▇▆▆▆▆▄▄▅▅▇█▆▆</td></tr><tr><td>agent_type_total_tag_count (agent:3, agent type:1)</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▃▄▄▃▄▄▄▃▄▅▅▆▆▆▆▆▇▇▇▆▅▅▅█▆▅▄</td></tr><tr><td>agent_type_total_tag_count (agent:4, agent type:0)</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▂▃▃▂▃▄▅▅▅▅▅▅▇▇▇▇▆▆▆▆▄▅▆▆█▇▆</td></tr><tr><td>agent_type_total_tag_count (agent:4, agent type:1)</td><td>▁▁▁▁▃▃▂▂▃▂▃▃▃▃▃▂▂▂▂▄▃▅▅▅▅▅▃▃▅▄▄▅▅▄▅▆▇▅▇█</td></tr><tr><td>team_blocks_laid (agent:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_laid (agent:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_laid (agent:2)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_laid (agent:3)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_laid (agent:4)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_mined (agent:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_mined (agent:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_mined (agent:2)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_mined (agent:3)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_blocks_mined (agent:4)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_flag_captures (agent:0)</td><td>▁▁▁▂▂▂▂▃▂▃▆▇██▇█▇▇▅▅▆▄▄▃▆▅▅▆▅▃▅▅▄▃▃▄▃▃▃▅</td></tr><tr><td>team_flag_captures (agent:1)</td><td>▁▁▂▂▃▃▂▃▃▄▅▅█▅▆▅█▆▆▆▆▆▆▆▅▅▆▇▆▅▇▇▇▆▇▅▆▆▆▆</td></tr><tr><td>team_flag_captures (agent:2)</td><td>▁▁▁▁▃▃▄▇▇▇▆█▆▇▇▆▆▆▆▆▅▅▇▇▅▆▆▅▇▇▆▆▄▄▅▆▇▆▆█</td></tr><tr><td>team_flag_captures (agent:3)</td><td>▁▁▂▁▂▂▃▃▄▃▄▅▄▆▄▅▅▅▆▆▅▆▇▇▇█▇█▇▆▆▇▆▅▆▇▇▆▇▆</td></tr><tr><td>team_flag_captures (agent:4)</td><td>▁▁▁▁▂▂▁▁▁▂▂▂▃▂▂▃▂▃▄▅▇▆█▆▅▇▇▇█▇▆▆▆▄▆▇▆▇▇▆</td></tr><tr><td>team_health_pickups (agent:0)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_health_pickups (agent:1)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_health_pickups (agent:2)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_health_pickups (agent:3)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_health_pickups (agent:4)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>team_respawn_tag_count (agent:0)</td><td>▁▁▁▂▂▂▃▃▃▄▄▅▆▇▆▅▅▅▅▅▆▅▅▅█▇▇▇▆▅▇▇▆▅▅▆▆▆▆▇</td></tr><tr><td>team_respawn_tag_count (agent:1)</td><td>▁▁▂▂▂▂▂▂▃▄▄▄▅▅▆▄▅▆▅▅▅▆▅▆▆▆▇▇▆▅▆▇▇▇▇▆██▇█</td></tr><tr><td>team_respawn_tag_count (agent:2)</td><td>▁▂▁▂▂▂▂▂▃▄▄▄▄▄▄▃▄▃▃▃▃▃▄▅▄▅▅▅▆▅▅▅▃▃▄▅█▇▅█</td></tr><tr><td>team_respawn_tag_count (agent:3)</td><td>▁▁▁▁▂▂▂▃▃▂▃▃▃▄▃▃▃▄▄▄▄▄▅▆▆▇▇▇▇▇▇▇▅▅▅▅██▆▅</td></tr><tr><td>team_respawn_tag_count (agent:4)</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▂▃▄▅▅▅▆▅▅▇▆▆▆▆▆▆▆▄▆▇▇███</td></tr><tr><td>team_total_distance_to_opp_flag (agent:0)</td><td>▄▆█▆▅▄▆▃▆▄▄▃▃▃▂▁▁▁▆█▄▄▃▄▃▅▅▂▃█▃▄▃▅▆▂█▇▇▃</td></tr><tr><td>team_total_distance_to_opp_flag (agent:1)</td><td>▇█▅▅▇▆▅▆▄▃▄▄▂▂▃▁▃▃▃▄▄▅▄▆▇█▅▅▇▃▃▄▃▄▆█▅▅▆▆</td></tr><tr><td>team_total_distance_to_opp_flag (agent:2)</td><td>▇▇██▄▅▇▅▁▂▃▃▅▄▄▄▃▃▂▃▄▄▅▅█▇▄▆▇▄▆▆▄▅▃▅▆▆▃▆</td></tr><tr><td>team_total_distance_to_opp_flag (agent:3)</td><td>█▇▇█▅▇█▄▁▅▃▂▄▅▅▃▁▄▃▄▃▃▃▅▆▅▆▅▆▇▅▄▃▃▂▃▅▄▃▃</td></tr><tr><td>team_total_distance_to_opp_flag (agent:4)</td><td>▇███▇▂██▇▆▅▇▄█▅▃▅▂▂▅▁▆▄▆▆▄▄▂▃▁▂▅▄▃▅▄▇▄██</td></tr><tr><td>team_total_distance_to_own_flag (agent:0)</td><td>▄▃▁▃▅▅▅▇▄▆▅▆▆▆▇███▅▄▇▇▇▆█▆▆█▇▅▇▆▇▆▆█▅▅▅█</td></tr><tr><td>team_total_distance_to_own_flag (agent:1)</td><td>▁▁▄▄▃▃▄▃▅▆▆▅▇▇▆█▆▆▆▆▇▆▇▆▅▆▇▇▆██▇▇█▇▅▆█▇▇</td></tr><tr><td>team_total_distance_to_own_flag (agent:2)</td><td>▁▂▁▂▃▃▂▃▄▄▄▄▄▄▄▅▅▅▆▅▆▆▅▅▄▅▅▃▄▆▅▅▇█▇▅▄▅▆▅</td></tr><tr><td>team_total_distance_to_own_flag (agent:3)</td><td>▁▂▂▃▅▃▃▆▇▆▇▇▇▆▆▇█▆▇▆▇▇▇▆▅▆▅▆▅▆▇▇▇██▇▆▇██</td></tr><tr><td>team_total_distance_to_own_flag (agent:4)</td><td>▁▁▁▂▃▆▄▃▄▆▆▅▆▄▇▇▆█▇▅█▅▆▅▆▇▇█▇██▆▇▇▇▆▅▆▄▄</td></tr><tr><td>team_total_tag_count (agent:0)</td><td>▁▁▁▂▂▂▂▃▃▃▄▅▆▆▆▆▆▆▅▅▆▅▅▅█▇▇▇▆▄▇▇▆▅▅▆▅▆▆▇</td></tr><tr><td>team_total_tag_count (agent:1)</td><td>▁▁▂▂▁▂▂▂▂▃▃▄▅▅▅▄▅▆▅▄▅▅▅▆▆▆▆▇▅▅▇▇▇▆▇▆▇█▇█</td></tr><tr><td>team_total_tag_count (agent:2)</td><td>▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▃▄▃▃▃▃▃▅▅▄▅▅▅▆▆▅▆▄▃▅▅█▇▅█</td></tr><tr><td>team_total_tag_count (agent:3)</td><td>▁▁▁▁▂▂▂▂▃▂▃▃▃▄▃▃▃▃▄▄▄▄▅▆▆▇▇▇▇▇▆▇▅▅▅▆██▆▆</td></tr><tr><td>team_total_tag_count (agent:4)</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▂▃▄▅▅▅▆▅▅▇▇▆▇▆▆▇▆▄▅▆▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_type_blocks_laid (agent:0, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:0, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:1, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:1, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:2, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:2, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:3, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:3, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:4, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_laid (agent:4, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:0, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:0, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:1, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:1, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:2, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:2, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:3, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:3, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:4, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_blocks_mined (agent:4, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_flag_captures (agent:0, agent type:0)</td><td>9.06667</td></tr><tr><td>agent_type_flag_captures (agent:0, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_flag_captures (agent:1, agent type:0)</td><td>11.8</td></tr><tr><td>agent_type_flag_captures (agent:1, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_flag_captures (agent:2, agent type:0)</td><td>14.2</td></tr><tr><td>agent_type_flag_captures (agent:2, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_flag_captures (agent:3, agent type:0)</td><td>11.53333</td></tr><tr><td>agent_type_flag_captures (agent:3, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_flag_captures (agent:4, agent type:0)</td><td>10.86667</td></tr><tr><td>agent_type_flag_captures (agent:4, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:0, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:0, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:1, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:1, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:2, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:2, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:3, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:3, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:4, agent type:0)</td><td>0.0</td></tr><tr><td>agent_type_health_pickups (agent:4, agent type:1)</td><td>0.0</td></tr><tr><td>agent_type_respawn_tag_count (agent:0, agent type:0)</td><td>11.13333</td></tr><tr><td>agent_type_respawn_tag_count (agent:0, agent type:1)</td><td>8.4</td></tr><tr><td>agent_type_respawn_tag_count (agent:1, agent type:0)</td><td>13.43333</td></tr><tr><td>agent_type_respawn_tag_count (agent:1, agent type:1)</td><td>11.26667</td></tr><tr><td>agent_type_respawn_tag_count (agent:2, agent type:0)</td><td>11.03333</td></tr><tr><td>agent_type_respawn_tag_count (agent:2, agent type:1)</td><td>13.06667</td></tr><tr><td>agent_type_respawn_tag_count (agent:3, agent type:0)</td><td>10.36667</td></tr><tr><td>agent_type_respawn_tag_count (agent:3, agent type:1)</td><td>8.0</td></tr><tr><td>agent_type_respawn_tag_count (agent:4, agent type:0)</td><td>9.53333</td></tr><tr><td>agent_type_respawn_tag_count (agent:4, agent type:1)</td><td>15.73333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:0, agent type:0)</td><td>2775.46667</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:0, agent type:1)</td><td>1586.03333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:1, agent type:0)</td><td>2747.53333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:1, agent type:1)</td><td>2526.03333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:2, agent type:0)</td><td>2663.03333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:2, agent type:1)</td><td>2291.76667</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:3, agent type:0)</td><td>2589.83333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:3, agent type:1)</td><td>1673.03333</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:4, agent type:0)</td><td>2762.3</td></tr><tr><td>agent_type_total_distance_to_opp_flag (agent:4, agent type:1)</td><td>2905.83333</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:0, agent type:0)</td><td>1927.46667</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:0, agent type:1)</td><td>3341.46667</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:1, agent type:0)</td><td>2003.73333</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:1, agent type:1)</td><td>2944.63333</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:2, agent type:0)</td><td>2334.36667</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:2, agent type:1)</td><td>2549.13333</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:3, agent type:0)</td><td>2054.96667</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:3, agent type:1)</td><td>3386.33333</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:4, agent type:0)</td><td>1891.96667</td></tr><tr><td>agent_type_total_distance_to_own_flag (agent:4, agent type:1)</td><td>1937.5</td></tr><tr><td>agent_type_total_tag_count (agent:0, agent type:0)</td><td>53.43333</td></tr><tr><td>agent_type_total_tag_count (agent:0, agent type:1)</td><td>18.4</td></tr><tr><td>agent_type_total_tag_count (agent:1, agent type:0)</td><td>67.96667</td></tr><tr><td>agent_type_total_tag_count (agent:1, agent type:1)</td><td>22.26667</td></tr><tr><td>agent_type_total_tag_count (agent:2, agent type:0)</td><td>57.43333</td></tr><tr><td>agent_type_total_tag_count (agent:2, agent type:1)</td><td>25.93333</td></tr><tr><td>agent_type_total_tag_count (agent:3, agent type:0)</td><td>54.03333</td></tr><tr><td>agent_type_total_tag_count (agent:3, agent type:1)</td><td>16.63333</td></tr><tr><td>agent_type_total_tag_count (agent:4, agent type:0)</td><td>53.3</td></tr><tr><td>agent_type_total_tag_count (agent:4, agent type:1)</td><td>29.83333</td></tr><tr><td>team_blocks_laid (agent:0)</td><td>0.0</td></tr><tr><td>team_blocks_laid (agent:1)</td><td>0.0</td></tr><tr><td>team_blocks_laid (agent:2)</td><td>0.0</td></tr><tr><td>team_blocks_laid (agent:3)</td><td>0.0</td></tr><tr><td>team_blocks_laid (agent:4)</td><td>0.0</td></tr><tr><td>team_blocks_mined (agent:0)</td><td>0.0</td></tr><tr><td>team_blocks_mined (agent:1)</td><td>0.0</td></tr><tr><td>team_blocks_mined (agent:2)</td><td>0.0</td></tr><tr><td>team_blocks_mined (agent:3)</td><td>0.0</td></tr><tr><td>team_blocks_mined (agent:4)</td><td>0.0</td></tr><tr><td>team_flag_captures (agent:0)</td><td>9.06667</td></tr><tr><td>team_flag_captures (agent:1)</td><td>11.8</td></tr><tr><td>team_flag_captures (agent:2)</td><td>14.2</td></tr><tr><td>team_flag_captures (agent:3)</td><td>11.53333</td></tr><tr><td>team_flag_captures (agent:4)</td><td>10.86667</td></tr><tr><td>team_health_pickups (agent:0)</td><td>0.0</td></tr><tr><td>team_health_pickups (agent:1)</td><td>0.0</td></tr><tr><td>team_health_pickups (agent:2)</td><td>0.0</td></tr><tr><td>team_health_pickups (agent:3)</td><td>0.0</td></tr><tr><td>team_health_pickups (agent:4)</td><td>0.0</td></tr><tr><td>team_respawn_tag_count (agent:0)</td><td>19.53333</td></tr><tr><td>team_respawn_tag_count (agent:1)</td><td>24.7</td></tr><tr><td>team_respawn_tag_count (agent:2)</td><td>24.1</td></tr><tr><td>team_respawn_tag_count (agent:3)</td><td>18.36667</td></tr><tr><td>team_respawn_tag_count (agent:4)</td><td>25.26667</td></tr><tr><td>team_total_distance_to_opp_flag (agent:0)</td><td>4361.5</td></tr><tr><td>team_total_distance_to_opp_flag (agent:1)</td><td>5273.56667</td></tr><tr><td>team_total_distance_to_opp_flag (agent:2)</td><td>4954.8</td></tr><tr><td>team_total_distance_to_opp_flag (agent:3)</td><td>4262.86667</td></tr><tr><td>team_total_distance_to_opp_flag (agent:4)</td><td>5668.13333</td></tr><tr><td>team_total_distance_to_own_flag (agent:0)</td><td>5268.93333</td></tr><tr><td>team_total_distance_to_own_flag (agent:1)</td><td>4948.36667</td></tr><tr><td>team_total_distance_to_own_flag (agent:2)</td><td>4883.5</td></tr><tr><td>team_total_distance_to_own_flag (agent:3)</td><td>5441.3</td></tr><tr><td>team_total_distance_to_own_flag (agent:4)</td><td>3829.46667</td></tr><tr><td>team_total_tag_count (agent:0)</td><td>71.83333</td></tr><tr><td>team_total_tag_count (agent:1)</td><td>90.23333</td></tr><tr><td>team_total_tag_count (agent:2)</td><td>83.36667</td></tr><tr><td>team_total_tag_count (agent:3)</td><td>70.66667</td></tr><tr><td>team_total_tag_count (agent:4)</td><td>83.13333</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Agent metrics logging</strong> at: <a href='https://wandb.ai/geoffrey-nightingale/MARL-CTF-Test/runs/xutlrauw' target=\"_blank\">https://wandb.ai/geoffrey-nightingale/MARL-CTF-Test/runs/xutlrauw</a><br/>Synced 6 W&B file(s), 24 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230423_064943-xutlrauw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(args.seed)\n",
    "\n",
    "# Initialise environment\n",
    "env = GridworldCtf(**args.env_config)\n",
    "\n",
    "dims_data = env.get_env_dims()\n",
    "local_grid_dims = dims_data[0]\n",
    "global_grid_dims = dims_data[1]\n",
    "local_metadata_dims = dims_data[2]\n",
    "global_metadata_dims = dims_data[3]\n",
    "n_channels = local_grid_dims[0]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Initialise agent population\n",
    "#----------------------------------------------------------------------\n",
    "main_agents = []\n",
    "exploiters = []\n",
    "historical_agents = []\n",
    "\n",
    "for _ in range(args.n_main_agents):\n",
    "    main_agents.append(Agent(n_channels, env.GRID_SIZE, local_metadata_dims[0]).to(args.device))\n",
    "\n",
    "for _ in range(args.n_exploiters):\n",
    "    exploiters.append(Agent(n_channels, env.GRID_SIZE, local_metadata_dims[0]).to(args.device))\n",
    "\n",
    "# Setup PPO trainer\n",
    "ppotrainer = PPOTrainer(args, local_grid_dims, local_metadata_dims)\n",
    "\n",
    "if args.use_wandb_selfplay:\n",
    "    wandb.init(project=args.wandb_project_name,\n",
    "                name=args.exp_name,\n",
    "                config=vars(args))\n",
    "\n",
    "# Setup metrics logger\n",
    "metlog = MetricsLogger(args.n_main_agents)\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "# League Training Start\n",
    "#----------------------------------------------------------------------\n",
    "for iteration in range(args.number_of_iterations):\n",
    "\n",
    "    for agent_idx, agent in enumerate(main_agents):\n",
    "        #TODO: Update to select multiple opponents and return as a list\n",
    "        # Select agent to train and opponent\n",
    "        opponent = select_opponent(iteration, agent_idx, main_agents, exploiters, historical_agents)\n",
    "\n",
    "        #TODO: Update train_ppo method to accept a list of opponents\n",
    "        # Train PPO\n",
    "        ppotrainer.train_ppo(args, env, agent, opponent)\n",
    "        clear_output()\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    # Duelling Phase and harvest of metrics\n",
    "    #----------------------------------------------------------------------\n",
    "    print(f'Iteration: {iteration} Duelling...')\n",
    "    all_agents = main_agents + exploiters + historical_agents\n",
    "    win_rate_dict = {i:0 for i in range(len(main_agents))}\n",
    "    for agent_idx, agent in enumerate(main_agents):\n",
    "        for opponent_idx, opponent in enumerate(all_agents):\n",
    "            if agent_idx != opponent_idx:\n",
    "                # print(f'agent {agent_idx} vs opponent {opponent_idx}')\n",
    "                for _ in range(args.number_of_duels):\n",
    "                    scaling_factor = 1 / (len(all_agents) - 1 + args.number_of_duels)\n",
    "                    agent_score, opponent_score = duel(env, agent, opponent)\n",
    "                    if agent_score > opponent_score:\n",
    "                        win_rate_dict[agent_idx] += 1 / (args.number_of_duels * len(all_agents)-1)\n",
    "\n",
    "                    # Log metrics -> track for main agents\n",
    "                    metlog.harvest_metrics(env, agent_idx, iteration, scaling_factor)\n",
    "\n",
    "    print(f'Win rates:\\n{win_rate_dict}')\n",
    "\n",
    "    # Log metrics\n",
    "    if args.use_wandb_selfplay:\n",
    "        metlog.log_to_wandb()\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    # Update agent pools\n",
    "    #----------------------------------------------------------------------\n",
    "    if iteration % args.main_agent_update_interval == 0:\n",
    "        print(f'Iteration: {iteration} Updating main agents')\n",
    "        update_main_agents(main_agents, win_rate_dict, max_win_rate_diff=args.max_win_rate_diff)\n",
    "\n",
    "    if iteration % args.exploiter_update_interval == 0:\n",
    "        print(f'Iteration: {iteration} Updating exploiters')\n",
    "        update_exploiters(main_agents, exploiters)\n",
    "\n",
    "    if iteration % args.historical_update_interval == 0:\n",
    "        print(f'Iteration: {iteration} Updating historical agent pool')\n",
    "        update_historical_agents(main_agents, historical_agents, win_rate_dict, max_pool_size=args.n_historical_agents)\n",
    "\n",
    "# Close wandb session\n",
    "if args.use_wandb_selfplay:\n",
    "    metlog.log_matplotlib_plots_to_wandb()\n",
    "    # metlog.log_wandb_table_plots()\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Trained Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = main_agents[0]\n",
    "opponent = main_agents[2]\n",
    "duel(env, agent, opponent, render=True, sleep_time=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_env_metadata_local(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move 0\n",
      "Game exited\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7, 1: 2}\n",
      "defaultdict(<class 'int'>, {1: 1, 0: 6})\n",
      "defaultdict(<class 'int'>, {3: 2, 2: 1, 0: 6})\n"
     ]
    }
   ],
   "source": [
    "print(env.metrics['team_flag_captures'])\n",
    "print(env.metrics['agent_type_flag_captures'][0])\n",
    "print(env.metrics['agent_flag_captures'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
